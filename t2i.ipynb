{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d52392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import gc\n",
    "\n",
    "from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9002d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory_detailed():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA不可用\")\n",
    "        return\n",
    "    device = torch.cuda.current_device()\n",
    "    props = torch.cuda.get_device_properties(device)\n",
    "    \n",
    "    # 1. 硬件总显存\n",
    "    total_gb = props.total_memory / (1024**3)\n",
    "    # 2. 已主动分配的显存（模型/张量占用）\n",
    "    allocated_gb = torch.cuda.memory_allocated(device) / (1024**3)\n",
    "    # 3. 缓存显存（PyTorch预留但未使用的“闲置缓存”）\n",
    "    cached_gb = torch.cuda.memory_reserved(device) / (1024**3)\n",
    "    # 4. 实际可用显存 = 总显存 - 已分配 - 缓存\n",
    "    actual_free_gb = total_gb - allocated_gb - cached_gb\n",
    "    \n",
    "    print(f\"GPU型号：{props.name}\")\n",
    "    print(f\"硬件总显存：{total_gb:.2f} GB\")\n",
    "    print(f\"已分配显存（模型/张量）：{allocated_gb:.2f} GB\")\n",
    "    print(f\"缓存显存（PyTorch预留）：{cached_gb:.2f} GB\")\n",
    "    print(f\"实际可立即使用的显存：{actual_free_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6028a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型选择：使用了 Stable Diffusion v1.5，这是一个平衡了质量和速度的模型，适合移动版 GPU 运行\n",
    "# 优化设置：\n",
    "# 使用 FP16 精度减少显存占用\n",
    "# 集成了 DPMSolver 调度器加速生成过程\n",
    "# 支持 xFormers 加速（需要额外安装：pip install xformers）\n",
    "# 禁用梯度计算减少显存使用\n",
    "# 参数调整：\n",
    "# 对于 RTX 5090 移动版，建议初始分辨率设置为 768x512 或 512x512\n",
    "# 推理步数 25-30 步可以在质量和速度间取得平衡\n",
    "# 引导尺度 7.5 左右可以很好地遵循提示词\n",
    "\n",
    "# 如果想尝试更先进的模型，可以将model_id改为：\n",
    "# \"stabilityai/stable-diffusion-2-1\"（更高质量）\n",
    "# \"runwayml/stable-diffusion-inpainting\"（支持图像修复）\n",
    "# 若遇到显存不足问题，可以：\n",
    "# 降低图像分辨率\n",
    "# 减少推理步数\n",
    "# 添加pipe.enable_attention_slicing()启用注意力切片\n",
    "# 对于更快的生成速度，可以尝试：\n",
    "# 使用 \"CompVis/stable-diffusion-v1-4\" 等较小模型\n",
    "# 减少推理步数至 20 步\n",
    "def generate_image(prompt, output_path=\"output.png\", model_id=\"black-forest-labs/FLUX.1-dev\", \n",
    "                   width=512, height=512, num_inference_steps=25, guidance_scale=7.5):\n",
    "    \"\"\"\n",
    "    使用Stable Diffusion生成图像\n",
    "    \n",
    "    参数:\n",
    "        prompt: 文本提示词\n",
    "        output_path: 输出图像路径\n",
    "        model_id: 模型ID\n",
    "        width: 生成图像宽度\n",
    "        height: 生成图像高度\n",
    "        num_inference_steps: 推理步数\n",
    "        guidance_scale: 引导尺度，值越大越遵循提示词\n",
    "    \"\"\"\n",
    "    # 检查CUDA是否可用\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    if device != \"cuda\":\n",
    "        print(\"警告: 未检测到CUDA，将使用CPU运行，速度会很慢\")\n",
    "        return\n",
    "    # 加载模型并配置调度器\n",
    "    pipe = DiffusionPipeline.from_pretrained(\"stabilityai/sd-turbo\", \n",
    "    torch_dtype=torch.float16,  # 半精度（22GB显存可不用，但加了更高效）\n",
    "    use_safetensors=True,       # 优先用safetensors（比ckpt更稳定）\n",
    "    safety_checker=None         # 保持你的需求\n",
    "    )\n",
    "    \n",
    "    # 使用DPMSolver调度器加速生成\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    \n",
    "    # 移动模型到GPU\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    # 启用xFormers加速（如果安装了xFormers）\n",
    "    try:\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"已启用xFormers加速\")\n",
    "    except:\n",
    "        print(\"未安装xFormers，将使用默认注意力机制\")\n",
    "    \n",
    "    # 生成图像\n",
    "    with torch.no_grad():  # 禁用梯度计算节省显存\n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale\n",
    "        ).images[0]\n",
    "    \n",
    "    # 保存图像\n",
    "    image.save(output_path)\n",
    "    print(f\"图像已保存至: {output_path}\")\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9505e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59ed1e310b143a586e51d536692149c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未安装xFormers，将使用默认注意力机制\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef1d79ba6294e4c9c8d4a59c3f012bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像已保存至: outputs_images/sunset.png\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   7450 MiB |   8364 MiB | 369423 MiB | 361973 MiB |\n",
      "|       from large pool |   7251 MiB |   8165 MiB | 356019 MiB | 348768 MiB |\n",
      "|       from small pool |    198 MiB |    207 MiB |  13403 MiB |  13205 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   7450 MiB |   8364 MiB | 369423 MiB | 361973 MiB |\n",
      "|       from large pool |   7251 MiB |   8165 MiB | 356019 MiB | 348768 MiB |\n",
      "|       from small pool |    198 MiB |    207 MiB |  13403 MiB |  13205 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   7389 MiB |   8303 MiB | 363956 MiB | 356567 MiB |\n",
      "|       from large pool |   7190 MiB |   8104 MiB | 350557 MiB | 343366 MiB |\n",
      "|       from small pool |    198 MiB |    206 MiB |  13399 MiB |  13200 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   9228 MiB |   9228 MiB |  10408 MiB |   1180 MiB |\n",
      "|       from large pool |   9016 MiB |   9016 MiB |  10190 MiB |   1174 MiB |\n",
      "|       from small pool |    212 MiB |    212 MiB |    218 MiB |      6 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  83837 KiB | 287791 KiB | 256068 MiB | 255986 MiB |\n",
      "|       from large pool |  80384 KiB | 285184 KiB | 241096 MiB | 241018 MiB |\n",
      "|       from small pool |   3453 KiB |  11277 KiB |  14971 MiB |  14968 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    3926    |    3957    |  107143    |  103217    |\n",
      "|       from large pool |    1069    |    1086    |   50209    |   49140    |\n",
      "|       from small pool |    2857    |    2880    |   56934    |   54077    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    3926    |    3957    |  107143    |  103217    |\n",
      "|       from large pool |    1069    |    1086    |   50209    |   49140    |\n",
      "|       from small pool |    2857    |    2880    |   56934    |   54077    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     415    |     415    |     432    |      17    |\n",
      "|       from large pool |     309    |     309    |     323    |      14    |\n",
      "|       from small pool |     106    |     106    |     109    |       3    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      61    |      77    |   52267    |   52206    |\n",
      "|       from large pool |      38    |      50    |   29762    |   29724    |\n",
      "|       from small pool |      23    |      34    |   22505    |   22482    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "GPU型号：NVIDIA GeForce RTX 5090 Laptop GPU\n",
      "硬件总显存：23.89 GB\n",
      "已分配显存（模型/张量）：4.85 GB\n",
      "缓存显存（PyTorch预留）：9.01 GB\n",
      "实际可立即使用的显存：10.03 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "prompt = \"a beautiful sunset over the mountains, detailed, vibrant colors, 8k resolution\"\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(\"outputs_images\", exist_ok=True)\n",
    "\n",
    "# 生成图像\n",
    "generate_image(\n",
    "    prompt=prompt,\n",
    "    model_id=\"black-forest-labs/FLUX.1-dev\", \n",
    "    output_path=\"outputs_images/sunset.png\",\n",
    "    width=768,  # RTX 5090移动版可以流畅处理这个分辨率\n",
    "    height=512,\n",
    "    num_inference_steps=30\n",
    ")\n",
    "\n",
    "check_gpu_memory_detailed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
