{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3e615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd31d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,math\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from functools import partial\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from scipy import linalg, integrate\n",
    "from datasets import load_dataset\n",
    "from diffusers.models.attention_processor import Attention\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4141b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# autoencoder\n",
    "def conv(ni, nf, ks=3, stride=2, act=True):\n",
    "    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res\n",
    "\n",
    "def deconv(ni, nf, ks=3, act=True):\n",
    "    layers = [nn.ConvTranspose2d(ni, nf, kernel_size=ks, stride=2, padding=ks//2)]\n",
    "    if act: layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "ae = nn.Sequential(   #28x28\n",
    "    nn.ZeroPad2d(2),  #32x32\n",
    "    conv(1,2),        #16x16\n",
    "    conv(2,4),        #8x8\n",
    "#     conv(4,8),        #4x4\n",
    "#     deconv(8,4),      #8x8\n",
    "    deconv(4,2),      #16x16\n",
    "    deconv(2,1, act=False), #32x32\n",
    "    nn.ZeroPad2d(-2), #28x28\n",
    "    nn.Sigmoid()\n",
    ").to(def_device)\n",
    "\n",
    "ni,nh,nl = 784,400,200\n",
    "def lin(ni, nf, act=nn.SiLU, norm=nn.BatchNorm1d, bias=True):\n",
    "    layers = nn.Sequential(nn.Linear(ni, nf, bias=bias))\n",
    "    if act : layers.append(act())\n",
    "    if norm: layers.append(norm(nf))\n",
    "    return layers\n",
    "\n",
    "def init_weights(m, leaky=0.):\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): init.kaiming_normal_(m.weight, a=leaky)\n",
    "\n",
    "iw = partial(init_weights, leaky=0.2)\n",
    "\n",
    "class Autoenc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(lin(ni, nh), lin(nh, nh), lin(nh, nl))\n",
    "        self.dec = nn.Sequential(lin(nl, nh), lin(nh, nh), lin(nh, ni, act=None))\n",
    "        iw(self)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        return self.dec(x)\n",
    "\n",
    "# VAE (Variational ae), z is latent variable\n",
    "class VAE(nn.Module): \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim): \n",
    "        super(VAE, self).__init__() \n",
    "        # ç¼–ç å™¨éƒ¨åˆ† \n",
    "        self.fc1  = nn.Linear(input_dim, hidden_dim) \n",
    "        self.fc_mu  = nn.Linear(hidden_dim, latent_dim) \n",
    "        self.fc_logvar  = nn.Linear(hidden_dim, latent_dim) \n",
    "        # è§£ç å™¨éƒ¨åˆ† \n",
    "        self.fc2  = nn.Linear(latent_dim, hidden_dim) \n",
    "        self.fc3  = nn.Linear(hidden_dim, input_dim) \n",
    "\n",
    "    def encode(self, x): \n",
    "        h = torch.relu(self.fc1(x))  \n",
    "        mu = self.fc_mu(h)  \n",
    "        logvar = self.fc_logvar(h)  \n",
    "        return mu, logvar \n",
    " \n",
    "    def reparameterize(self, mu, logvar): \n",
    "        std = torch.exp(0.5  * logvar) \n",
    "        eps = torch.randn_like(std)  \n",
    "        return mu + eps * std \n",
    " \n",
    "    def decode(self, z): \n",
    "        h = torch.relu(self.fc2(z))  \n",
    "        return torch.sigmoid(self.fc3(h))  \n",
    " \n",
    "    def forward(self, x): \n",
    "        mu, logvar = self.encode(x)  \n",
    "        z = self.reparameterize(mu,  logvar) \n",
    "        return self.decode(z),  mu, logvar \n",
    "\n",
    "class VAE2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(lin(ni, nh), lin(nh, nh))\n",
    "        self.mu,self.lv = lin(nh, nl, act=None),lin(nh, nl, act=None)\n",
    "        self.dec = nn.Sequential(lin(nl, nh), lin(nh, nh), lin(nh, ni, act=None))\n",
    "        iw(self)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        mu,lv = self.mu(x),self.lv(x)\n",
    "        z = mu + (0.5*lv).exp()*torch.randn_like(lv)\n",
    "        return self.dec(z),mu,lv\n",
    "\n",
    "# Kullback-Leibler Divergence Loss\n",
    "def kld_loss(inp, x):\n",
    "    x_hat,mu,lv = inp\n",
    "    return -0.5 * (1 + lv - mu.pow(2) - lv.exp()).mean()\n",
    "# Binary Cross-Entropy Loss\n",
    "def bce_loss(inp, x): return F.binary_cross_entropy_with_logits(inp[0], x)\n",
    "\n",
    "def vae_loss(inp, x): return kld_loss(inp, x) + bce_loss(inp,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb4cb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x -= self.sub\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "def noop (x=None, *args, **kwargs):\n",
    "    \"Do nothing\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "\n",
    "def _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n",
    "    return nn.Sequential(conv(ni, nf, stride=1, act=act, norm=norm, ks=ks),\n",
    "                         conv(nf, nf, stride=stride, act=None, norm=norm, ks=ks))\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "        super().__init__()\n",
    "        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n",
    "        self.idconv = noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x): return self.act(self.convs(x) + self.idconv(self.pool(x)))\n",
    "\n",
    "def get_model(act=nn.ReLU, nfs=(8,16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers).to(def_device)\n",
    "\n",
    "# ResNetç±»çš„åŸºæœ¬ç»“æ„\n",
    "# ResNetç±»é€šå¸¸ç»§æ‰¿è‡ªPyTorchçš„nn.ModuleåŸºç±»ï¼ŒåŒ…å«__init__åˆå§‹åŒ–æ–¹æ³•å’Œforwardå‰å‘ä¼ æ’­æ–¹æ³•ã€‚åœ¨__init__ä¸­å®šä¹‰ç½‘ç»œå„å±‚ç»“æ„ï¼Œforwardæ–¹æ³•åˆ™æè¿°æ•°æ®åœ¨ç½‘ç»œä¸­çš„æµåŠ¨é¡ºåºã€‚\n",
    "# ResNetä¸»è¦ç”±åˆå§‹å·ç§¯å±‚ã€æ‰¹é‡å½’ä¸€åŒ–å±‚ã€ReLUæ¿€æ´»å‡½æ•°ã€æœ€å¤§æ± åŒ–å±‚ã€4ä¸ªæ®‹å·®å—å±‚ï¼ˆlayer1-layer4ï¼‰ã€å¹³å‡æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ç»„æˆã€‚\n",
    "# æ®‹å·®å—å®ç°\n",
    "# ResNetåŒ…å«ä¸¤ç§æ®‹å·®å—ï¼šBasicBlockå’ŒBottleneckã€‚BasicBlockç”¨äºè¾ƒæµ…çš„ç½‘ç»œï¼ˆå¦‚ResNet18/34ï¼‰ï¼Œç”±ä¸¤ä¸ª3x3å·ç§¯å±‚ç»„æˆï¼›Bottleneckç”¨äºæ·±å±‚ç½‘ç»œï¼ˆå¦‚ResNet50/101/152ï¼‰ï¼Œç”±1x1ã€3x3ã€1x1ä¸‰ä¸ªå·ç§¯å±‚æ„æˆï¼Œ\n",
    "# ç¬¬ä¸€ä¸ª1x1å·ç§¯ç”¨äºé™ç»´ï¼Œæœ€åä¸€ä¸ª1x1å·ç§¯æ¢å¤ç»´åº¦ã€‚æ¯ä¸ªå·ç§¯å±‚åéƒ½è·Ÿéšæ‰¹é‡å½’ä¸€åŒ–å’ŒReLUæ¿€æ´»ã€‚æ®‹å·®è¿æ¥é€šè¿‡å°†è¾“å…¥ç›´æ¥åŠ åˆ°è¾“å‡ºä¸Šæ¥å®ç°ï¼Œå½“ç»´åº¦ä¸åŒ¹é…æ—¶ä½¿ç”¨1x1å·ç§¯è¿›è¡Œä¸‹é‡‡æ ·ã€‚\n",
    "# forwardæ–¹æ³•å®ç°\n",
    "# forwardæ–¹æ³•å®šä¹‰äº†æ•°æ®æµåŠ¨è·¯å¾„ï¼šå…ˆé€šè¿‡åˆå§‹å·ç§¯å±‚ï¼ˆconv1ï¼‰ã€æ‰¹é‡å½’ä¸€åŒ–ï¼ˆbn1ï¼‰ã€ReLUå’Œæœ€å¤§æ± åŒ–ï¼ˆmaxpoolï¼‰ï¼Œç„¶åä¾æ¬¡é€šè¿‡4ä¸ªæ®‹å·®å—å±‚ï¼ˆlayer1-layer4ï¼‰ï¼Œæœ€åç»è¿‡å¹³å‡æ± åŒ–ï¼ˆavgpoolï¼‰ã€å±•å¹³ï¼ˆflattenï¼‰å’Œå…¨è¿æ¥å±‚ï¼ˆfcï¼‰ã€‚\n",
    "# åœ¨PyTorchä¸­ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡å®ä¾‹è°ƒç”¨ï¼ˆå¦‚model(input)ï¼‰æ¥è‡ªåŠ¨è§¦å‘forwardæ–¹æ³•ï¼Œè¿™å¾—ç›ŠäºPythonçš„__call__æœºåˆ¶ã€‚\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1 \n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1  = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1  = nn.BatchNorm2d(planes)\n",
    "        self.conv2  = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2  = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut  = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes: \n",
    "            self.shortcut  = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,  kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes) \n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x))) \n",
    "        out = self.bn2(self.conv2(out)) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        return out \n",
    " \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes  = 64 \n",
    "        \n",
    "        self.conv1  = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1  = nn.BatchNorm2d(64)\n",
    "        self.maxpool  = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1  = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2  = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3  = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4  = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        self.avgpool  = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc  = nn.Linear(512*block.expansion,  num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes,  planes, stride))\n",
    "            self.in_planes  = planes * block.expansion  \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))) \n",
    "        x = self.maxpool(x) \n",
    "        \n",
    "        x = self.layer1(x) \n",
    "        x = self.layer2(x) \n",
    "        x = self.layer3(x) \n",
    "        x = self.layer4(x) \n",
    "        \n",
    "        x = self.avgpool(x) \n",
    "        x = torch.flatten(x,  1)\n",
    "        x = self.fc(x) \n",
    "        return x \n",
    " \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ed9e6",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models(DDPM)\n",
    "The model described in the seminal 2020 paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM). For more context, while diffusion models were technically invented [back in 2015](https://arxiv.org/abs/1503.03585)\n",
    "\n",
    "DDPM is trained quite simply in a few steps:\n",
    "1. randomly select some timesteps in an iterative noising process.\n",
    "2. Add noise corresponding to this timestep to the original image. For increasing timesteps, the variance of the noise increases.\n",
    "3. Pass in this noisy image and the timestep to our model\n",
    "4. Model is trained with an MSE loss between the model output and the amount of noise added to the image\n",
    "\n",
    "# DDPM æ ¸å¿ƒåŸç†ç®—æ³•è¯¦è§£ \n",
    " \n",
    "**Denoising Diffusion Probabilistic Models (DDPM)** æ˜¯ä¸€ç§åŸºäºé©¬å°”å¯å¤«é“¾çš„ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é€æ­¥åŠ å™ªå’Œå»å™ªè¿‡ç¨‹å®ç°æ•°æ®ç”Ÿæˆã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒåŸç†ç®—æ³•ï¼š\n",
    " \n",
    "## ä¸€ã€å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼ˆåŠ å™ªï¼‰\n",
    "å°†åŸå§‹æ•°æ® $x_0$ é€æ­¥æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œç»è¿‡ $T$ æ­¥è½¬åŒ–ä¸ºçº¯å™ªå£° $x_T$ï¼š\n",
    " \n",
    "$$\n",
    "q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\n",
    "è¡¨ç¤ºåœ¨ç»™å®šå‰ä¸€æ­¥çŠ¶æ€x_{t-1}çš„æ¡ä»¶ä¸‹ï¼Œå½“å‰çŠ¶æ€x_{t}çš„åˆ†å¸ƒæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ã€‚å…·ä½“å‚æ•°å¦‚ä¸‹ï¼š\\\\\n",
    "å‡å€¼ï¼ˆmeanï¼‰ï¼š \\sqrt{1-\\beta_t}x_{t-1}  \n",
    "æ–¹å·®ï¼ˆvarianceï¼‰ï¼š\\beta_t Iï¼ŒIæ˜¯å•ä½çŸ©é˜µï¼Œè¡¨ç¤ºå„ä¸ªç»´åº¦ç‹¬ç«‹ä¸”æ–¹å·®ç›¸åŒ\n",
    "$$\n",
    "### 1. å‡å€¼åˆ†é‡ï¼š$\\sqrt{1-\\beta_t}x_{t-1}$\n",
    "| **ç‰¹æ€§**        | **æ•°å­¦æ„ä¹‰**               | **ç‰©ç†æ„ä¹‰**                     |\n",
    "|-----------------|--------------------------|----------------------------------|\n",
    "| ç¼©å‡å› å­        | $\\sqrt{1-\\beta_t} < 1$   | å¯¹å‰æ­¥æ•°æ®çš„ç¼©æ”¾è¡°å‡             |\n",
    "| ä¿¡å·ä¿ç•™        | ä¸$\\beta_t$æˆåæ¯”         | $\\beta_t$è¶Šå¤§ï¼Œä¿¡å·ä¿ç•™è¶Šå°‘      |\n",
    "| ç¨³å®šæ€§ä¿éšœ      | é˜²æ­¢æ•°æ®å¹…åº¦æ— é™å¢é•¿       | ç»´æŒæ‰©æ•£è¿‡ç¨‹çš„æ•°å€¼ç¨³å®š           |\n",
    " \n",
    "### 2. æ–¹å·®åˆ†é‡ï¼š$\\beta_t I$\n",
    "| **ç‰¹æ€§**        | **æ•°å­¦æ„ä¹‰**               | **ç‰©ç†æ„ä¹‰**                     |\n",
    "|-----------------|--------------------------|----------------------------------|\n",
    "| å„å‘åŒæ€§        | $I$ ä¸ºå•ä½çŸ©é˜µ           | å„ç»´åº¦ç‹¬ç«‹åŒåˆ†å¸ƒå™ªå£°             |\n",
    "| å™ªå£°å¼ºåº¦        | $\\beta_t$ æ§åˆ¶æ–¹å·®å¤§å°    | å†³å®šå½“å‰æ­¥æ·»åŠ çš„å™ªå£°é‡           |\n",
    "| æ—¶é—´ä¾èµ–æ€§      | $\\beta_t$ éš $t$ å•è°ƒé€’å¢ | éšæ—¶é—´æ­¥å¢åŠ å™ªå£°é€æ­¥å¢å¼º         |\n",
    " \n",
    "## âš™ï¸ å‚æ•°è§£é‡Š\n",
    "### $\\beta_t$ - å™ªå£°è°ƒåº¦å‚æ•° \n",
    "- **å–å€¼èŒƒå›´**ï¼š$(0,1)$  \n",
    "- **è°ƒåº¦ç‰¹æ€§**ï¼š  \n",
    "  $$\\beta_1 < \\beta_2 < \\cdots < \\beta_T$$  \n",
    "  ï¼ˆå…¸å‹å€¼ï¼š$\\beta_1â‰ˆ0.0001$, $\\beta_Tâ‰ˆ0.02$ï¼‰\n",
    "- **ä½œç”¨æœºåˆ¶**ï¼š  \n",
    "  æ§åˆ¶å™ªå£°æ³¨å…¥é€Ÿç‡ï¼Œå†³å®šæ‰©æ•£è½¨è¿¹å½¢çŠ¶ \n",
    " \n",
    "### $1-\\beta_t$ - ä¿¡å·ä¿ç•™å› å­\n",
    "- **ç‰©ç†ä½œç”¨**ï¼š  \n",
    "  å¹³è¡¡ä¿¡å·è¡°å‡ä¸å™ªå£°æ³¨å…¥çš„æ¯”ä¾‹å…³ç³»  \n",
    "  $$\\text{ä¿¡å·ä¿ç•™é‡} \\propto \\sqrt{1-\\beta_t}$$  \n",
    "  $$\\text{å™ªå£°æ³¨å…¥é‡} \\propto \\sqrt{\\beta_t}$$\n",
    "\n",
    "\n",
    "**å…³é”®ç‰¹æ€§ï¼š**\n",
    "1. **é—­å¼è¡¨è¾¾**ï¼šä»»æ„æ—¶åˆ» $t$ çš„çŠ¶æ€å¯ç›´æ¥ä» $x_0$ è®¡ç®—ï¼š\n",
    "   $$\n",
    "   x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)\n",
    "   $$\n",
    "   å…¶ä¸­ $\\alpha_t = 1-\\beta_t$, $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n",
    " \n",
    "2. **å™ªå£°è°ƒåº¦**ï¼š$\\beta_t$ æ»¡è¶³ $0 < \\beta_1 < \\beta_2 < \\cdots < \\beta_T < 1$ï¼Œé€šå¸¸é‡‡ç”¨çº¿æ€§æˆ–ä½™å¼¦è°ƒåº¦ \n",
    " \n",
    "## äºŒã€åå‘å»å™ªè¿‡ç¨‹ï¼ˆç”Ÿæˆï¼‰\n",
    "å­¦ä¹ ä»å™ªå£° $x_T$ é€æ­¥é‡å»ºæ•°æ® $x_0$ çš„é€†è¿‡ç¨‹ï¼š\n",
    " \n",
    "$$\n",
    "p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n",
    "$$\n",
    " \n",
    "**å‚æ•°åŒ–å½¢å¼ï¼š**\n",
    "$$\n",
    "\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t,t) \\right)\n",
    "$$\n",
    "$$\n",
    "\\Sigma_\\theta(x_t,t) = \\sigma_t^2 I, \\quad \\sigma_t^2 = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t\n",
    "$$\n",
    " \n",
    "## ä¸‰ã€è®­ç»ƒç›®æ ‡ï¼ˆç®€åŒ–æŸå¤±å‡½æ•°ï¼‰\n",
    "ä¼˜åŒ–å™ªå£°é¢„æµ‹ç½‘ç»œ $\\epsilon_\\theta$ï¼š\n",
    " \n",
    "$$\n",
    "\\mathcal{L}_{simple} = \\mathbb{E}_{t,x_0,\\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t) \\|^2 \\right]\n",
    "$$\n",
    " \n",
    "**è®­ç»ƒæµç¨‹ï¼š**\n",
    "1. é‡‡æ ·æ•°æ® $x_0 \\sim q(x_0)$ \n",
    "2. éšæœºé€‰æ‹©æ—¶é—´æ­¥ $t \\sim \\text{Uniform}(1,T)$ \n",
    "3. é‡‡æ ·å™ªå£° $\\epsilon \\sim \\mathcal{N}(0,I)$\n",
    "4. è®¡ç®—æ¢¯åº¦ä¸‹é™æ›´æ–° $\\nabla_\\theta \\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2$\n",
    "\n",
    "## ğŸ“Œ æ ¸å¿ƒé‡‡æ ·å…¬å¼\n",
    "$$\n",
    "x_{t-1} = \\underbrace{\\mu_\\theta(x_t, t)}_{\\text{ç¡®å®šæ€§æˆåˆ†}} + \\underbrace{\\sigma_t \\cdot z}_{\\text{éšæœºæˆåˆ†}},\\quad z \\sim \\mathcal{N}(0,I)\n",
    "$$\n",
    " \n",
    "## ğŸ” åŒç»„åˆ†ç‰©ç†æ„ä¹‰è§£æ\n",
    "### ğŸ¯ ç¡®å®šæ€§æˆåˆ† $\\mu_\\theta(x_t, t)$\n",
    "| **ç‰¹æ€§**       | **æ•°å­¦æœ¬è´¨**                  | **ç”Ÿæˆä½œç”¨**                     |\n",
    "|----------------|-----------------------------|--------------------------------|\n",
    "| å»å™ªæ–¹å‘é¢„æµ‹   | ç¥ç»ç½‘ç»œ $\\epsilon_\\theta$ çš„è¾“å‡ºæ˜ å°„ | æŒ‡å‘æ•°æ®æµå½¢çš„æœ€ä¼˜é‡å»ºè·¯å¾„        |\n",
    "| ä¿¡æ¯å‹ç¼©       | $\\mu_\\theta = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta)$ | ä¿ç•™æœ‰æ•ˆä¿¡å·ï¼Œæ»¤é™¤é«˜é¢‘å™ªå£°        |\n",
    "| åˆ†å¸ƒé”šç‚¹       | è¿‘ä¼¼çœŸå®åéªŒåˆ†å¸ƒ $q(x_{t-1}\\|x_t,x_0)$ çš„å‡å€¼ | ç¡®ä¿ç”Ÿæˆæ ·æœ¬ä½äºæ•°æ®åˆ†å¸ƒé«˜æ¦‚ç‡åŒºåŸŸ | \n",
    "\n",
    "## å››ã€é‡‡æ ·ç®—æ³•ï¼ˆç”Ÿæˆæ–°æ ·æœ¬ï¼‰\n",
    "```python\n",
    "def ddpm_sampling(model, T):\n",
    "    x_T = torch.randn_like(data)   # çº¯å™ªå£° \n",
    "    for t in range(T, 0, -1):     # ä»Tåˆ°1åå‘è¿­ä»£\n",
    "        z = torch.randn_like(x_T)  if t > 1 else 0\n",
    "        # è®¡ç®—å™ªå£°é¢„æµ‹\n",
    "        eps_theta = model(x_T, t) \n",
    "        # è®¡ç®—å‡å€¼ \n",
    "        mu_theta = (x_T - (beta_t/sqrt(1-alpha_bar_t))*eps_theta) / sqrt(alpha_t)\n",
    "        # æ›´æ–°æ ·æœ¬ \n",
    "        x_{t-1} = mu_theta + sigma_t * z\n",
    "    return x_0  # ç”Ÿæˆæ ·æœ¬ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn  as nn \n",
    "import torch.optim  as optim\n",
    "import numpy as np \n",
    "from torch.utils.data  import DataLoader \n",
    "from torchvision import datasets, transforms \n",
    "from torchvision.utils  import save_image\n",
    "import matplotlib.pyplot  as plt\n",
    "from tqdm import tqdm\n",
    " \n",
    "# ======================\n",
    "# é…ç½®å‚æ•° \n",
    "# ======================\n",
    "device = torch.device(\"cuda\"  if torch.cuda.is_available()  else \"cpu\")\n",
    "batch_size = 64\n",
    "image_size = 32  # CIFAR-10æ ‡å‡†å°ºå¯¸\n",
    "num_epochs = 100\n",
    "T = 1000  # æ‰©æ•£æ­¥æ•°\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    " \n",
    "# ======================\n",
    "# å™ªå£°è°ƒåº¦å™¨ \n",
    "# ======================\n",
    "# cosine schedule\n",
    "def abar(t): return (t*math.pi/2).cos()**2\n",
    "def inv_abar(x): return x.sqrt().acos()*2/math.pi\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.rand(n,).to(x0).clamp(0,0.999)\n",
    "    Îµ = torch.randn(x0.shape, device=device)\n",
    "    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*Îµ\n",
    "    return (xt, t.to(device)), Îµ\n",
    "\n",
    "class NoiseScheduler:\n",
    "    def __init__(self, T, beta_start, beta_end):\n",
    "        self.T = T \n",
    "        self.betas  = torch.linspace(beta_start,  beta_end, T).to(device)\n",
    "        self.alphas  = 1. - self.betas  \n",
    "        self.alpha_bars  = torch.cumprod(self.alphas,  dim=0)\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0) \n",
    "        \n",
    "        alpha_bar_t = self.alpha_bars[t].view(-1,  1, 1, 1)\n",
    "        noisy_x = torch.sqrt(alpha_bar_t)  * x_0 + torch.sqrt(1  - alpha_bar_t) * noise \n",
    "        return noisy_x, noise\n",
    " \n",
    "# ======================\n",
    "# U-Netæ¨¡å‹æ¶æ„ \n",
    "# ======================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp  = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv  = nn.ConvTranspose2d(in_ch, out_ch, 3, padding=1)\n",
    "        else:\n",
    "            self.conv  = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm  = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        h = self.conv(x) \n",
    "        time_emb = self.act(self.time_mlp(t)) \n",
    "        h = h + time_emb[:, :, None, None]\n",
    "        h = self.norm(h) \n",
    "        return self.act(h) \n",
    " \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        channels = [64, 128, 256, 512]\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥ \n",
    "        self.time_mlp  = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        # ä¸‹é‡‡æ ·è·¯å¾„ \n",
    "        self.down1  = Block(in_channels, channels[0], 32)\n",
    "        self.down2  = Block(channels[0], channels[1], 32)\n",
    "        self.down3  = Block(channels[1], channels[2], 32)\n",
    "        \n",
    "        # æœ€åº•å±‚\n",
    "        self.bottleneck  = Block(channels[2], channels[3], 32)\n",
    "        \n",
    "        # ä¸Šé‡‡æ ·è·¯å¾„ \n",
    "        self.up1  = Block(channels[3] + channels[2], channels[2], 32, up=True)\n",
    "        self.up2  = Block(channels[2] + channels[1], channels[1], 32, up=True)\n",
    "        self.up3  = Block(channels[1] + channels[0], channels[0], 32, up=True)\n",
    "        \n",
    "        # è¾“å‡ºå±‚ \n",
    "        self.out  = nn.Conv2d(channels[0], out_channels, 1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # æ—¶é—´åµŒå…¥\n",
    "        t = t.unsqueeze(-1).float() \n",
    "        t_emb = self.time_mlp(t) \n",
    "        \n",
    "        # ä¸‹é‡‡æ ·\n",
    "        d1 = self.down1(x,  t_emb)\n",
    "        d2 = self.down2(d1,  t_emb)\n",
    "        d3 = self.down3(d2,  t_emb)\n",
    "        \n",
    "        # ç“¶é¢ˆå±‚ \n",
    "        bottleneck = self.bottleneck(d3,  t_emb)\n",
    "        \n",
    "        # ä¸Šé‡‡æ ·\n",
    "        u1 = self.up1(torch.cat([bottleneck,  d3], dim=1), t_emb)\n",
    "        u2 = self.up2(torch.cat([u1,  d2], dim=1), t_emb)\n",
    "        u3 = self.up3(torch.cat([u2,  d1], dim=1), t_emb)\n",
    "        \n",
    "        return self.out(u3) \n",
    " \n",
    "# ======================\n",
    "# è®­ç»ƒå‡½æ•° \n",
    "# ======================\n",
    "def train(model, dataloader, optimizer, scheduler, epochs=T):\n",
    "    model.train() \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 \n",
    "        for images, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device) \n",
    "            \n",
    "            # éšæœºé‡‡æ ·æ—¶é—´æ­¥\n",
    "            t = torch.randint(0,  T, (images.shape[0],)).to(device) \n",
    "            \n",
    "            # æ·»åŠ å™ªå£° \n",
    "            noisy_images, noise = scheduler.add_noise(images,  t)\n",
    "            \n",
    "            # é¢„æµ‹å™ªå£° \n",
    "            predicted_noise = model(noisy_images, t)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤± \n",
    "            loss = loss_fn(predicted_noise, noise)\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            \n",
    "            total_loss += loss.item() \n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(),  f\"ddpm_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    return model\n",
    " \n",
    "# ======================\n",
    "# é‡‡æ ·ç”Ÿæˆå›¾åƒ \n",
    "# ======================\n",
    "def sample(model, scheduler, n_images=16):\n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        # ä»éšæœºå™ªå£°å¼€å§‹\n",
    "        x = torch.randn(n_images,  3, image_size, image_size).to(device)\n",
    "        \n",
    "        for t_step in tqdm(reversed(range(T)), desc=\"Generating\"):\n",
    "            t = torch.full((n_images,),  t_step, device=device, dtype=torch.long) \n",
    "            \n",
    "            # é¢„æµ‹å™ªå£°\n",
    "            pred_noise = model(x, t)\n",
    "            \n",
    "            # è®¡ç®—ç³»æ•° \n",
    "            alpha = scheduler.alphas[t][:,  None, None, None]\n",
    "            alpha_bar = scheduler.alpha_bars[t][:,  None, None, None]\n",
    "            beta = scheduler.betas[t][:,  None, None, None]\n",
    "            \n",
    "            # è®¡ç®—é‡æ–°å‚æ•°åŒ–çš„å‡å€¼\n",
    "            if t_step > 0:\n",
    "                noise = torch.randn_like(x) \n",
    "            else:\n",
    "                noise = torch.zeros_like(x) \n",
    "                \n",
    "            # DDPMé‡‡æ ·å…¬å¼\n",
    "            x = (1 / torch.sqrt(alpha))  * (\n",
    "                x - ((1 - alpha) / (torch.sqrt(1  - alpha_bar))) * pred_noise\n",
    "            ) + torch.sqrt(beta)  * noise \n",
    "            \n",
    "        # å°†å›¾åƒç¼©æ”¾åˆ°[0,1]èŒƒå›´\n",
    "        x = (x.clamp(-1,  1) + 1) / 2 \n",
    "        save_image(x, \"generated_samples.png\",  nrow=4)\n",
    "        return x\n",
    "\n",
    "class MixedPrecision():\n",
    "    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler('cuda')\n",
    "    def before_batch(self, learn):\n",
    "        self.autocast = torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "        self.autocast.__enter__()\n",
    "    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n",
    "    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n",
    "    def step(self, learn):\n",
    "        self.scaler.step(learn.opt)\n",
    "        self.scaler.update()\n",
    "\n",
    "# ======================\n",
    "# ä¸»ç¨‹åº \n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. å‡†å¤‡æ•°æ® \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # 2. åˆå§‹åŒ–ç»„ä»¶ \n",
    "    noise_scheduler = NoiseScheduler(T, beta_start, beta_end)\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),  lr=1e-4)\n",
    "    \n",
    "    # 3. è®­ç»ƒæ¨¡å‹ \n",
    "    print(\"Starting training...\")\n",
    "    trained_model = train(model, dataloader, optimizer, noise_scheduler, num_epochs)\n",
    "    \n",
    "    # 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    torch.save(trained_model.state_dict(),  \"ddpm_final.pth\") \n",
    "    print(\"Training completed. Model saved.\")\n",
    "    \n",
    "    # 5. ç”Ÿæˆæ ·æœ¬ \n",
    "    print(\"Generating samples...\")\n",
    "    generated_images = sample(trained_model, noise_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4903d17",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Implicit Models - DDIM\n",
    "\n",
    "# DDIMï¼ˆDenoising Diffusion Implicit Modelsï¼‰æ ¸å¿ƒåŸç†ä¸ç®—æ³• \n",
    " \n",
    "## 1. æ ¸å¿ƒæ€æƒ³ \n",
    "DDIM æ˜¯æ‰©æ•£æ¨¡å‹çš„æ”¹è¿›å½¢å¼ï¼Œé€šè¿‡**éé©¬å°”å¯å¤«é“¾çš„ç¡®å®šæ€§è¿‡ç¨‹**å®ç°é«˜è´¨é‡ç”Ÿæˆï¼ŒåŒæ—¶æ”¯æŒ**å¤§å¹…å‡å°‘é‡‡æ ·æ­¥éª¤**ï¼ˆå¦‚10-50æ­¥ï¼‰ã€‚å…¶å…³é”®çªç ´åœ¨äºï¼š\n",
    "- å°†æ‰©æ•£è¿‡ç¨‹é‡æ–°å‚æ•°åŒ–ä¸ºå¯é€†çš„éé©¬å°”å¯å¤«é“¾ \n",
    "- é€šè¿‡éšå¼æ¦‚ç‡æ¨¡å‹å®ç°ç¡®å®šæ€§é‡‡æ · \n",
    "- ä¿æŒä¸DDPMç›¸åŒçš„è®­ç»ƒç›®æ ‡ï¼ˆå¯å¤ç”¨å·²æœ‰æ¨¡å‹ï¼‰\n",
    " \n",
    "## 2. æ ¸å¿ƒç®—æ³•æ­¥éª¤ \n",
    " \n",
    "### 2.1 å‰å‘è¿‡ç¨‹ï¼ˆéé©¬å°”å¯å¤«ï¼‰\n",
    "å®šä¹‰ä»»æ„æ—¶é—´åºåˆ—å­é›† $\\{Ï„_1,Ï„_2,...,Ï„_T\\}$ï¼Œå‰å‘è¿‡ç¨‹ä¸ºï¼š\n",
    "$$\n",
    "q_Ïƒ(x_{1:T}|x_0) = q_Ïƒ(x_T|x_0)\\prod_{t=2}^T q_Ïƒ(x_{Ï„_{t-1}}|x_{Ï„_t},x_0)\n",
    "$$\n",
    "å…¶ä¸­ $Ïƒ$ æ§åˆ¶éšæœºæ€§ï¼ˆå½“ $Ïƒ=0$ æ—¶ä¸ºç¡®å®šæ€§è¿‡ç¨‹ï¼‰\n",
    " \n",
    "### 2.2 åå‘è¿‡ç¨‹ï¼ˆå…³é”®å…¬å¼ï¼‰\n",
    "é‡‡æ ·å…¬å¼ï¼š\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\barÎ±_{t-1}} \\left( \\frac{x_t - \\sqrt{1-\\barÎ±_t}Îµ_Î¸(x_t,t)}{\\sqrt{\\barÎ±_t}} \\right) + \\sqrt{1-\\barÎ±_{t-1}-Ïƒ_t^2}Â·Îµ_Î¸(x_t,t) + Ïƒ_tÎµ \n",
    "$$\n",
    "å…¶ä¸­ï¼š\n",
    "- $Îµ_Î¸(x_t,t)$ï¼šè®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹ç½‘ç»œ \n",
    "- $\\barÎ±_t$ï¼šç´¯ç§¯å™ªå£°è°ƒåº¦å‚æ•° \n",
    "- $Ïƒ_t$ï¼šæ§åˆ¶éšæœºæ€§çš„è¶…å‚æ•° \n",
    " \n",
    "## 3. å…³é”®æ•°å­¦æ¨å¯¼ \n",
    "### 3.1 å˜åˆ†ä¸‹ç•Œé‡æ„ \n",
    "ä¿æŒä¸DDPMç›¸åŒçš„è®­ç»ƒç›®æ ‡ï¼š\n",
    "$$\n",
    "L_{Î¸} = \\mathbb{E}_{t,x_0,Îµ}[||Îµ - Îµ_Î¸(x_t,t)||^2]\n",
    "$$\n",
    " \n",
    "### 3.2 ç¡®å®šæ€§é‡‡æ ·ï¼ˆÏƒ=0ï¼‰\n",
    "å½“ $Ïƒ_t=0$ æ—¶ï¼Œé‡‡æ ·è¿‡ç¨‹å˜ä¸ºï¼š\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\barÎ±_{t-1}}f_Î¸(x_t,t) + \\sqrt{1-\\barÎ±_{t-1}}Îµ_Î¸(x_t,t)\n",
    "$$\n",
    "å…¶ä¸­ $f_Î¸(x_t,t) = (x_t - \\sqrt{1-\\barÎ±_t}Îµ_Î¸(x_t,t))/\\sqrt{\\barÎ±_t}$\n",
    "\n",
    "### DDIM æ—¶é—´å­åºåˆ—é‡å‚æ•°åŒ–\n",
    "DDIM é€šè¿‡é‡æ„åŸå§‹æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥åºåˆ— $\\{1,2,...,T\\}$ ä¸ºä»»æ„å­åºåˆ— $\\{\\tau_1,\\tau_2,...,\\tau_S\\}$ï¼ˆæ»¡è¶³ $S \\ll T$ï¼‰ï¼Œå®ç°è·¨æ­¥è®¡ç®—ã€‚æ ¸å¿ƒå…¬å¼å¦‚ä¸‹ï¼š\n",
    " \n",
    "####  ğŸ§® è·¨æ­¥è®¡ç®—å…¬å¼ \n",
    "$$x_{\\tau_{i-1}} = \\sqrt{\\bar{\\alpha}_{\\tau_{i-1}}} \\left( \\frac{x_{\\tau_i} - \\sqrt{1-\\bar{\\alpha}_{\\tau_i}} \\epsilon_\\theta (x_{\\tau_i}, \\tau_i)} {\\sqrt{\\bar{\\alpha}_{\\tau_i}}} \\right) + \\sqrt{1-\\bar{\\alpha}_{\\tau_{i-1}}} \\epsilon_\\theta (x_{\\tau_i}, \\tau_i)$$\n",
    " \n",
    "####  ç´¯ç§¯å™ªå£°è°ƒåº¦\n",
    "$$\\bar{\\alpha}_{\\tau_{i-1}} = \\prod_{s=1}^{\\tau_{i-1}} (1-\\beta_s) \\quad \\text{(ç›´æ¥è·³è·ƒåˆ°ç›®æ ‡æ—¶é—´æ­¥)}$$\n",
    " \n",
    "---\n",
    " \n",
    "###  å…³é”®çªç ´ç‚¹\n",
    "1. **æ—¶é—´æ­¥è§£è€¦**  \n",
    "   - å­åºåˆ—æ­¥ $\\tau_i$ ä¸å‰ä¸€æ­¥ $\\tau_{i-1}$ çš„å…³ç³»**ä¸ä¾èµ–ä¸­é—´çŠ¶æ€**  \n",
    "   - ä»…é€šè¿‡ $\\bar{\\alpha}$ å‚æ•°å…³è”ï¼Œå®ç°è®¡ç®—è·¯å¾„çš„æ‹“æ‰‘è§£è€¦\n",
    " \n",
    "2. **ç´¯ç§¯å™ªå£°è°ƒåº¦**  \n",
    "   - $\\bar{\\alpha}$ çš„**æŒ‡æ•°è¡°å‡æ€§è´¨**æ˜¯è·¨æ­¥è®¡ç®—çš„æ ¸å¿ƒ  \n",
    "   - é€šè¿‡ä¹˜ç§¯å½¢å¼ $\\prod_{s=1}^{\\tau_{i-1}} (1-\\beta_s)$ ç›´æ¥å»ºæ¨¡ç´¯è®¡å™ªå£°å½±å“  \n",
    "   - å…è®¸åœ¨ä»»æ„å­åºåˆ—æ­¥é•¿é—´è¿›è¡Œç¡®å®šæ€§è·³è·ƒ\n",
    "\n",
    "## 4. ä¸DDPMçš„å…³é”®åŒºåˆ« \n",
    "| ç‰¹æ€§        | DDPM           | DDIM          |\n",
    "|-------------|----------------|---------------|\n",
    "| è¿‡ç¨‹ç±»å‹    | é©¬å°”å¯å¤«é“¾     | éé©¬å°”å¯å¤«é“¾  |\n",
    "| é‡‡æ ·æ–¹å¼    | éšæœºè¿‡ç¨‹       | ç¡®å®šæ€§è¿‡ç¨‹    |\n",
    "| é‡‡æ ·æ­¥æ•°    | é€šå¸¸éœ€è¦1000+  | å¯ä½è‡³10-50æ­¥ |\n",
    "| ç”Ÿæˆè½¨è¿¹    | å”¯ä¸€ç¡®å®š       | å¤šè·¯å¾„å¯èƒ½    |\n",
    "| è®­ç»ƒå…¼å®¹æ€§  | åŸç”Ÿæ¨¡å‹       | å®Œå…¨å…¼å®¹      |\n",
    " \n",
    "## 5. ç®—æ³•ä¼˜åŠ¿ \n",
    "1. **åŠ é€Ÿé‡‡æ ·**ï¼šç›¸æ¯”DDPMæé€Ÿ20-100å€ \n",
    "2. **è´¨é‡ä¿æŒ**ï¼šåœ¨10æ­¥é‡‡æ ·æ—¶ä»å¯ä¿æŒç”Ÿæˆè´¨é‡ \n",
    "3. **åºåˆ—æ’å€¼**ï¼šæ”¯æŒæ½œåœ¨ç©ºé—´æ’å€¼æ“ä½œ \n",
    "4. **å…¼å®¹å¤ç”¨**ï¼šå¯ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒDDPMæ¨¡å‹ \n",
    "5. **å¯æ§éšæœºæ€§**ï¼šé€šè¿‡Ïƒå‚æ•°è°ƒèŠ‚ç”Ÿæˆå¤šæ ·æ€§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from math import sqrt \n",
    " \n",
    "class DDIMSampler(nn.Module):\n",
    "    def __init__(self, model, timesteps=1000, ddim_steps=50, eta=0.0):\n",
    "        self.model  = model  # é¢„è®­ç»ƒå¥½çš„UNetæ¨¡å‹ \n",
    "        self.timesteps  = timesteps \n",
    "        self.ddim_steps  = ddim_steps  # å­åºåˆ—æ­¥æ•°(S)\n",
    "        self.eta  = eta  # éšæœºå› å­(Î·=0æ—¶ç¡®å®šæ€§ç”Ÿæˆ)\n",
    "        \n",
    "        # ç”Ÿæˆæ—¶é—´å­åºåˆ—(Ï„_1,Ï„_2,...,Ï„_S)\n",
    "        self.time_seq  = self._generate_time_sequence()\n",
    "        # é¢„è®¡ç®—ç´¯ç§¯å™ªå£°å‚æ•°Î±_bar\n",
    "        self.alphas_cumprod = model.alphas_cumprod\n",
    " \n",
    "    def _generate_time_sequence(self):\n",
    "        \"\"\"ç”Ÿæˆæ—¶é—´æ­¥å­åºåˆ—(ç­‰å·®æ•°åˆ—é‡‡æ ·)\"\"\"\n",
    "        step_ratio = self.timesteps  // self.ddim_steps  \n",
    "        return list(range(0, self.timesteps,  step_ratio))[::-1]\n",
    " \n",
    "    def sample_step(self, x, t, t_prev):\n",
    "        \"\"\"å•æ­¥å»å™ªè¿‡ç¨‹ [12]()\"\"\"\n",
    "        # è·å–å™ªå£°é¢„æµ‹æ¨¡å‹è¾“å‡º \n",
    "        eps_theta = self.model(x,  t)\n",
    "        \n",
    "        # æå–Î±_barå‚æ•° \n",
    "        alpha_bar = self.alphas_cumprod[t] \n",
    "        alpha_bar_prev = self.alphas_cumprod[t_prev] \n",
    "        \n",
    "        # è®¡ç®—é¢„æµ‹çš„x0 (å¼(12) [12]())\n",
    "        pred_x0 = (x - sqrt(1 - alpha_bar) * eps_theta) / sqrt(alpha_bar)\n",
    "        \n",
    "        # è®¡ç®—æ–¹å·®é¡¹ \n",
    "        sigma = self.eta  * sqrt( (1 - alpha_bar_prev)/(1 - alpha_bar) * (1 - alpha_bar/alpha_bar_prev) )\n",
    "        \n",
    "        # è®¡ç®—å‡å€¼é¡¹ \n",
    "        mean = sqrt(alpha_bar_prev) * pred_x0 + sqrt(1 - alpha_bar_prev - sigma**2) * eps_theta \n",
    "        \n",
    "        # ç”Ÿæˆå‰ä¸€æ­¥æ ·æœ¬ \n",
    "        if self.eta  == 0:\n",
    "            return mean \n",
    "        else:\n",
    "            return mean + sigma * torch.randn_like(x) \n",
    " \n",
    "    def ddim_sample(self, shape=(1,3,64,64)):\n",
    "        \"\"\"å®Œæ•´é‡‡æ ·æµç¨‹\"\"\"\n",
    "        x = torch.randn(shape).to(device) \n",
    "        \n",
    "        for i in range(len(self.time_seq)-1): \n",
    "            t = self.time_seq[i] \n",
    "            t_prev = self.time_seq[i+1] \n",
    "            x = self.sample_step(x,  t, t_prev)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8a14c",
   "metadata": {},
   "source": [
    "# ç‰›é¡¿-èˆ’å°”èŒ¨è¿­ä»£æ³•ï¼ˆNewton-Schulz Iterationï¼‰è¯¦è§£ \n",
    " \n",
    "## 1. åŸºæœ¬åŸç† \n",
    "ç‰›é¡¿-èˆ’å°”èŒ¨è¿­ä»£æ³•æ˜¯ç»å…¸ç‰›é¡¿è¿­ä»£æ³•åœ¨çŸ©é˜µè¿ç®—ä¸­çš„æ‰©å±•ï¼Œç”¨äºæ±‚è§£çŸ©é˜µå¹³æ–¹æ ¹é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸åŠ¨ç‚¹è¿­ä»£é€¼è¿‘çŸ©é˜µå¹³æ–¹æ ¹ï¼Œé¿å…äº†æ˜¾å¼çš„çŸ©é˜µåˆ†è§£å’Œæ±‚é€†æ“ä½œ\n",
    " \n",
    "### æ•°å­¦è¡¨è¾¾å¼ \n",
    "ç»™å®šéå¥‡å¼‚çŸ©é˜µ$A \\in \\mathbb{R}^{n \\times n}$ï¼Œå¯»æ‰¾çŸ©é˜µ$X$ä½¿å¾—ï¼š\n",
    "$$ X^2 = A $$\n",
    "ç‰›é¡¿-èˆ’å°”èŒ¨è¿­ä»£å…¬å¼ä¸ºï¼š\n",
    "$$ X_{k+1} = \\frac{1}{2} X_k (3I - X_k^2 A) $$\n",
    "å…¶ä¸­$X_0$ä¸ºåˆå§‹çŒœæµ‹çŸ©é˜µï¼Œé€šå¸¸å–$X_0 = A$æˆ–$X_0 = \\frac{A}{\\|A\\|_2}$ã€‚\n",
    "\n",
    "# ç”Ÿæˆæ¨¡å‹è¯„ä¼°æŒ‡æ ‡è¯¦è§£ï¼šFIDã€MMDä¸KID \n",
    "## 1. FIDï¼ˆFrÃ©chet Inception Distanceï¼‰\n",
    "### åŸç† \n",
    "- **æ•°å­¦åŸºç¡€**ï¼šè®¡ç®—ä¸¤ä¸ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»ï¼š\n",
    "  $$ \\text{FID} = \\|\\mu_1 - \\mu_2\\|^2 + \\text{Tr}(\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_1 \\Sigma_2)^{1/2}) $$\n",
    "  å…¶ä¸­ $\\mu$ ä¸ºå‡å€¼å‘é‡ï¼Œ$\\Sigma$ ä¸ºåæ–¹å·®çŸ©é˜µ[1]()ã€‚\n",
    "- **ç‰¹å¾æå–**ï¼šä½¿ç”¨Inception-v3æ¨¡å‹ï¼ˆç§»é™¤åˆ†ç±»å±‚ï¼‰æå–2048ç»´å›¾åƒç‰¹å¾å‘é‡ã€‚\n",
    "- **æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡ç‰¹å¾ç©ºé—´çš„ç»Ÿè®¡é‡ï¼ˆå‡å€¼ã€åæ–¹å·®ï¼‰é‡åŒ–åˆ†å¸ƒå·®å¼‚ï¼Œå€¼è¶Šå°è¡¨ç¤ºç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒè¶Šç›¸ä¼¼[2]()ã€‚\n",
    "### è®¡ç®—æ­¥éª¤ \n",
    "1. **ç‰¹å¾ç»Ÿè®¡**ï¼šè®¡ç®—çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒç‰¹å¾çš„å‡å€¼å‘é‡ä¸åæ–¹å·®çŸ©é˜µã€‚\n",
    "2. **çŸ©é˜µè¿ç®—**ï¼š\n",
    "   - è®¡ç®—åæ–¹å·®çŸ©é˜µä¹˜ç§¯çš„å¹³æ–¹æ ¹ï¼ˆ`linalg.sqrtm` ï¼‰\n",
    "   - ç»„åˆå‡å€¼å·®å¹³æ–¹ã€åæ–¹å·®è¿¹å’Œå¹³æ–¹æ ¹è¿¹ \n",
    "3. **è¾“å‡º**ï¼šæ ‡é‡å€¼ï¼ˆ`.item()`æå–ï¼‰ï¼Œå€¼è¶Šä½è¡¨ç¤ºç”Ÿæˆè´¨é‡è¶Šå¥½ã€‚\n",
    "### ç‰¹ç‚¹ä¸åº”ç”¨ \n",
    "- **ä¼˜åŠ¿**ï¼šä¸äººç±»è§†è§‰æ„ŸçŸ¥é«˜åº¦ä¸€è‡´ï¼ŒGANè¯„ä¼°çš„é»„é‡‘æ ‡å‡†[[1][2]()ã€‚\n",
    "- **å±€é™**ï¼šå‡è®¾ç‰¹å¾æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå¯¹éé«˜æ–¯åˆ†å¸ƒå¯èƒ½é«˜ä¼°å·®å¼‚ã€‚\n",
    "- **å…¸å‹åœºæ™¯**ï¼šStyleGANã€ProGANç­‰ç”Ÿæˆæ¨¡å‹çš„å…¨å±€åˆ†å¸ƒè¯„ä¼°ã€‚\n",
    "---\n",
    "## 2. MMDï¼ˆMaximum Mean Discrepancyï¼‰\n",
    "### åŸç† \n",
    "- **æ•°å­¦åŸºç¡€**ï¼šåŸºäºå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰çš„å‡å€¼å·®å¼‚ï¼š\n",
    "  $$ \\text{MMD}^2 = \\mathbb{E}[k(x,x')] + \\mathbb{E}[k(y,y')] - 2\\mathbb{E}[k(x,y)] $$\n",
    "- **æ ¸å‡½æ•°**ï¼šä»£ç é‡‡ç”¨ä¸‰æ¬¡å¤šé¡¹å¼æ ¸ $k(a,b) = (a \\cdot b^T / d + 1)^3$ï¼Œå…¶ä¸­ $d$ ä¸ºç‰¹å¾ç»´åº¦[7]()ã€‚\n",
    "- **æ— åä¼°è®¡**ï¼šå‰”é™¤æ ¸çŸ©é˜µå¯¹è§’çº¿å…ƒç´ ï¼ˆ`kxx_sum = kxx.sum()  - kxx.diagonal().sum()` ï¼‰ï¼Œé¿å…æ ·æœ¬è‡ªç›¸ä¼¼å¹²æ‰°ã€‚\n",
    "### è®¡ç®—æ­¥éª¤ \n",
    "1. **æ ¸çŸ©é˜µè®¡ç®—**ï¼šåŒç±»æ ·æœ¬æ ¸ï¼ˆ`kxx, kyy`ï¼‰ä¸è·¨ç±»æ ¸ï¼ˆ`kxy`ï¼‰ã€‚\n",
    "2. **ç»Ÿè®¡é‡ç»„åˆ**ï¼š\n",
    "   - $kxx$ å’Œ $kyy$ çš„å‰”é™¤å¯¹è§’çº¿å’Œ \n",
    "   - $kxy$ çš„å®Œæ•´å’Œ \n",
    "3. **æ ‡å‡†åŒ–è¾“å‡º**ï¼šæŒ‰æ ·æœ¬é‡åŠ æƒç»„åˆç»“æœã€‚\n",
    "### ç‰¹ç‚¹ä¸åº”ç”¨ \n",
    "- **ä¼˜åŠ¿**ï¼šéå‚æ•°æ–¹æ³•ï¼Œä¸ä¾èµ–æ•°æ®åˆ†å¸ƒå‡è®¾[7]()ã€‚\n",
    "- **çµæ´»æ€§**ï¼šå¯é€šè¿‡æ›´æ¢æ ¸å‡½æ•°ï¼ˆå¦‚é«˜æ–¯æ ¸ï¼‰é€‚åº”ä¸åŒä»»åŠ¡ã€‚\n",
    "- **å…¸å‹åœºæ™¯**ï¼šè¿ç§»å­¦ä¹ çš„åŸŸé€‚åº”ã€åŒæ ·æœ¬å‡è®¾æ£€éªŒã€‚\n",
    "---\n",
    "## 3. KIDï¼ˆKernel Inception Distanceï¼‰\n",
    "### åŸç† \n",
    "- **æ”¹è¿›MMD**ï¼šé€šè¿‡åˆ†å—è®¡ç®—MMDçš„å‡å€¼ï¼Œè§£å†³å¤§æ ·æœ¬è®¡ç®—ç“¶é¢ˆï¼š\n",
    "  $$ \\text{KID} = \\frac{1}{n}\\sum_{i=1}^n \\text{MMD}^2(S_{x,i}, S_{y,i}) $$\n",
    "- **åˆ†å—ç­–ç•¥**ï¼šé»˜è®¤å—å¤§å°â‰¤50ï¼Œæœ€å°åˆ†å—æ•°â‰¥4ï¼ˆ`n = max(ceil(min(xs/maxs, ys/maxs)), 4)`ï¼‰[6]()ã€‚\n",
    "- **æ— åæ€§**ï¼šä¸‰æ¬¡æ ¸çš„æ— åä¼°è®¡æ›´ç¬¦åˆäººç±»æ„ŸçŸ¥[[3][7]()ã€‚\n",
    "### è®¡ç®—æ­¥éª¤ \n",
    "1. **åŠ¨æ€åˆ†å—**ï¼šæ ¹æ®æ ·æœ¬é‡è®¡ç®—å­é›†æ•°é‡ $n$ã€‚\n",
    "2. **å­é›†MMDè®¡ç®—**ï¼šå¾ªç¯è®¡ç®—æ¯ä¸ªå­é›†çš„ `_squared_mmd`ã€‚\n",
    "3. **ç»“æœå¹³å‡**ï¼šè¾“å‡ºåˆ†å—MMDçš„å‡å€¼ï¼ˆ`mmd/n`ï¼‰ã€‚\n",
    "### ç‰¹ç‚¹ä¸åº”ç”¨ \n",
    "- **ä¼˜åŠ¿**ï¼šå¯¹å°æ ·æœ¬æ›´é²æ£’ï¼Œè®¡ç®—æ•ˆç‡é«˜äºFID[6]()ã€‚\n",
    "- **å…¸å‹åœºæ™¯**ï¼šè¯„ä¼°ç”Ÿæˆå›¾åƒçš„å±€éƒ¨ç‰¹å¾ä¸€è‡´æ€§ï¼ˆå¦‚StyleGANç»†èŠ‚çº¹ç†ï¼‰ã€‚\n",
    "---\n",
    "## æŒ‡æ ‡å¯¹æ¯”æ€»ç»“ \n",
    "| **æŒ‡æ ‡** | **ç†è®ºåŸºç¡€**       | **è®¡ç®—å¤æ‚åº¦** | **æ•°æ®å‡è®¾** | **æ ¸å¿ƒä¼˜åŠ¿**               |\n",
    "|----------|---------------------|----------------|--------------|----------------------------|\n",
    "| FID      | FrÃ©chetè·ç¦»         | é«˜ï¼ˆçŸ©é˜µåˆ†è§£ï¼‰ | é«˜æ–¯åˆ†å¸ƒ     | å…¨å±€åˆ†å¸ƒè¯„ä¼°               |\n",
    "| MMD      | æ ¸ç©ºé—´å‡å€¼å·®å¼‚      | ä¸­             | æ—            | éå‚æ•°ã€çµæ´»æ ¸å‡½æ•°         |\n",
    "| KID      | MMDåˆ†å—å¹³å‡         | ä½             | æ—            | å±€éƒ¨ç‰¹å¾ä¸€è‡´æ€§ã€å°æ ·æœ¬é²æ£’ |\n",
    "> **åº”ç”¨å»ºè®®**ï¼š  \n",
    "> - éœ€å¿«é€Ÿè¯„ä¼°å…¨å±€è´¨é‡ â†’ **FID**  \n",
    "> - å…³æ³¨å±€éƒ¨ç»†èŠ‚æˆ–æ— åˆ†å¸ƒå‡è®¾ â†’ **KID**  \n",
    "> - éå›¾åƒæ•°æ®æˆ–è¿ç§»å­¦ä¹  â†’ **MMD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sqrtm_newton_schulz(mat, num_iters=100):\n",
    "    mat_nrm = mat.norm()\n",
    "    mat = mat.double()\n",
    "    Y = mat/mat_nrm\n",
    "    n = len(mat)\n",
    "    I = torch.eye(n, n).to(mat)\n",
    "    Z = torch.eye(n, n).to(mat)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        T = (3*I - Z@Y)/2\n",
    "        Y,Z = Y@T,T@Z\n",
    "        res = Y*mat_nrm.sqrt()\n",
    "        if ((mat-(res@res)).norm()/mat_nrm).abs()<=1e-6: break\n",
    "    return res\n",
    "\n",
    "def _calc_stats(feats):\n",
    "    feats = feats.squeeze()\n",
    "    return feats.mean(0),feats.T.cov()\n",
    "\n",
    "def _calc_fid(m1,c1,m2,c2):\n",
    "#     csr = _sqrtm_newton_schulz(c1@c2)\n",
    "    csr = tensor(linalg.sqrtm(c1@c2, 256).real)\n",
    "    return (((m1-m2)**2).sum() + c1.trace() + c2.trace() - 2*csr.trace()).item()\n",
    "\n",
    "def _squared_mmd(x, y):\n",
    "    def k(a,b): return (a@b.transpose(-2,-1)/a.shape[-1]+1)**3\n",
    "    m,n = x.shape[-2],y.shape[-2]\n",
    "    kxx,kyy,kxy = k(x,x), k(y,y), k(x,y)\n",
    "    kxx_sum = kxx.sum([-1,-2])-kxx.diagonal(0,-1,-2).sum(-1)\n",
    "    kyy_sum = kyy.sum([-1,-2])-kyy.diagonal(0,-1,-2).sum(-1)\n",
    "    kxy_sum = kxy.sum([-1,-2])\n",
    "    return kxx_sum/m/(m-1) + kyy_sum/n/(n-1) - kxy_sum*2/m/n\n",
    "\n",
    "def _calc_kid(x, y, maxs=50):\n",
    "    xs,ys = x.shape[0],y.shape[0]\n",
    "    n = max(math.ceil(min(xs/maxs, ys/maxs)), 4)\n",
    "    mmd = 0.\n",
    "    for i in range(n):\n",
    "        cur_x = x[round(i*xs/n) : round((i+1)*xs/n)]\n",
    "        cur_y = y[round(i*ys/n) : round((i+1)*ys/n)]\n",
    "        mmd += _squared_mmd(cur_x, cur_y)\n",
    "    return (mmd/n).item()\n",
    "\n",
    "class ImageEval:\n",
    "    def __init__(self, model, dls, cbs=None):\n",
    "        self.learn = TrainLearner(model, dls, loss_func=fc.noop, cbs=cbs, opt_func=None)\n",
    "        self.feats = self.learn.capture_preds()[0].float().cpu().squeeze()\n",
    "        self.stats = _calc_stats(self.feats)\n",
    "\n",
    "    def get_feats(self, samp):\n",
    "        self.learn.dls = DataLoaders([],[(samp, tensor([0]))])\n",
    "        return self.learn.capture_preds()[0].float().cpu().squeeze()\n",
    "\n",
    "    def fid(self, samp): return _calc_fid(*self.stats, *_calc_stats(self.get_feats(samp)))\n",
    "    def kid(self, samp): return _calc_kid(self.feats, self.get_feats(samp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe32a8",
   "metadata": {},
   "source": [
    "# Karras pre-conditioning\n",
    "Karras Scalingç®—æ³•çš„æ ¸å¿ƒæ€æƒ³\n",
    "Karras Scalingç®—æ³•æ˜¯ä¸€ç§ç”¨äºç¨³å®šå’ŒåŠ é€Ÿç”Ÿæˆæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼‰è®­ç»ƒçš„æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åŠ¨æ€è°ƒæ•´ç½‘ç»œå±‚çš„è¾“å…¥è¾“å‡ºå°ºåº¦æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥ç®—æ³•åŸºäºå¯¹ç¥ç»ç½‘ç»œè®­ç»ƒåŠ¨æ€çš„æ·±å…¥åˆ†æï¼Œå‘ç°ä¸åŒå±‚çº§çš„æ¿€æ´»å€¼å°ºåº¦ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ”¶æ•›æ€§å’Œæœ€ç»ˆæ€§èƒ½ã€‚\n",
    "\n",
    "ç®—æ³•æ•°å­¦è¡¨è¾¾\n",
    "Karras Scalingç®—æ³•é€šè¿‡ä»¥ä¸‹å…¬å¼å®ç°ç½‘ç»œå±‚çš„å°ºåº¦æ§åˆ¶ï¼š å¯¹äºç»™å®šçš„ç¥ç»ç½‘ç»œå±‚ï¼Œè®¾æƒé‡çŸ©é˜µä¸ºWï¼Œåç½®å‘é‡ä¸ºbï¼Œåˆ™ç»è¿‡Scalingå˜æ¢åçš„å‚æ•°ä¸ºï¼š W' = Î»W b' = Î»b å…¶ä¸­Î»æ˜¯ä¸€ä¸ªä¸è®­ç»ƒæ—¶é—´æ­¥ç›¸å…³çš„æ¯”ä¾‹å› å­ï¼Œç”¨äºæ§åˆ¶Scalingçš„ç¨‹åº¦ã€‚è¿™ä¸ªæ¯”ä¾‹å› å­é€šå¸¸è®¾è®¡ä¸ºéšæ—¶é—´æ­¥å˜åŒ–çš„å‡½æ•°ï¼Œä½¿å¾—ç½‘ç»œåœ¨ä¸åŒè®­ç»ƒé˜¶æ®µå…·æœ‰ä¸åŒçš„åŠ¨æ€ç‰¹æ€§ã€‚\n",
    "\n",
    "# Karras Scalingç®—æ³•åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨è§£æ \n",
    " \n",
    "## æ—¶é—´æ¡ä»¶åŒ–å®ç°æœºåˆ¶ \n",
    "**æ ¸å¿ƒå…¬å¼**ï¼š  \n",
    "`fÎ¸(xt,t) = c_skip(t)x_t + c_out(t)FÎ¸(c_in(t)x_t, c_noise(t))`  \n",
    "\n",
    "**æŸå¤±å‡½æ•°loss**:\n",
    "\n",
    "![Various norms](../imgs/karras.png)\n",
    "\n",
    "**å…³é”®ç³»æ•°å‡½æ•°**ï¼š\n",
    "| å‡½æ•°       | ä½œç”¨æè¿°                           | è®¾è®¡ç›®æ ‡                     |\n",
    "|------------|----------------------------------|-----------------------------|\n",
    "| `c_skip(t)` | ä¿ç•™åŸå§‹è¾“å…¥çš„è·³è·ƒè¿æ¥ç³»æ•°          | ç»´æŒä¿¡å·å®Œæ•´æ€§                |\n",
    "| `c_out(t)`  | æ§åˆ¶ç½‘ç»œè¾“å‡ºçš„ç¼©æ”¾å› å­              | è°ƒèŠ‚é¢„æµ‹ä¿¡å·çš„å¼ºåº¦            |\n",
    "| `c_in(t)`   | è¾“å…¥ç‰¹å¾å½’ä¸€åŒ–ç³»æ•°                 | ç¨³å®šä¸åŒå™ªå£°æ°´å¹³çš„è¾“å…¥åˆ†å¸ƒ     |\n",
    "| `c_noise(t)`| æ—¶é—´åµŒå…¥å‘é‡çš„è°ƒèŠ‚ç³»æ•°             | åŠ¨æ€å¹³è¡¡æ—¶é—´æ¡ä»¶çš„å½±å“         |\n",
    " \n",
    "## æ‰©æ•£æ¨¡å‹ä¸­çš„å…·ä½“åº”ç”¨ \n",
    "1. **è¾“å…¥å½’ä¸€åŒ–**  \n",
    "   - é€šè¿‡`c_in(t)`åŠ¨æ€è°ƒæ•´å™ªå£°è¾“å…¥å°ºåº¦ï¼Œæ¶ˆé™¤ä¸åŒé‡‡æ ·æ­¥é—´çš„é‡çº§å·®å¼‚\n",
    " \n",
    "2. **è¾“å‡ºæ§åˆ¶**  \n",
    "   - `c_out(t)`å°†ç½‘ç»œè¾“å‡ºæ˜ å°„åˆ°åˆé€‚çš„ä¿¡å·èŒƒå›´ï¼Œé¿å…å¹…å€¼éœ‡è¡ \n",
    " \n",
    "3. **æ®‹å·®è¿æ¥**  \n",
    "   - `c_skip(t)`æ„å»ºçš„è·³è·ƒè¿æ¥ä¿ç•™ä½é¢‘ä¿¡æ¯ï¼Œé˜²æ­¢é«˜é¢‘ç»†èŠ‚ä¸¢å¤±\n",
    " \n",
    "4. **æ—¶é—´åµŒå…¥ä¼˜åŒ–**  \n",
    "   - `c_noise(t)`åŠ æƒæ—¶é—´ç¼–ç ï¼Œå¢å¼ºæ¨¡å‹å¯¹å™ªå£°è°ƒåˆ¶çš„æ•æ„Ÿæ€§ \n",
    "\n",
    "# Karras Scalingç³»æ•°ç³»ç»Ÿè¯¦è§£ï¼ˆ2025å¹´æœ€æ–°æ•´ç†ï¼‰\n",
    "å™ªå£°æ ·æœ¬çš„æ–¹å·® $Var(x_t)= \\sigma_t^2 Var(x_{0}) + \\alpha_t^2 Var(\\varepsilon) = \\sigma_t^2 + \\alpha_t^2$ å‡è®¾æ•°æ®å·²ç»æ ‡å‡†åŒ–\n",
    "\n",
    "## æ ¸å¿ƒç³»æ•°å®šä¹‰è¡¨ \n",
    "| ç³»æ•°          | æ•°å­¦å½¢å¼                          | ç‰©ç†æ„ä¹‰                                                                 | åŠ¨æ€èŒƒå›´åˆ†æ                |\n",
    "|---------------|----------------------------------|--------------------------------------------------------------------------|---------------------------|\n",
    "| **c_in(t)**   | $\\frac{1}{\\sqrt{\\sigma_t^2 + \\alpha_t^2}}$ | è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ–<br>â€¢ æ¶ˆé™¤é‡çº²å·®å¼‚<br>â€¢ ç¨³å®šåå‘ä¼ æ’­æ¢¯åº¦                    | éš$t$å¢å¤§å•è°ƒé€’å‡           |\n",
    "| **c_out(t)**  | $\\frac{\\sigma_t}{\\sqrt{\\sigma_t^2 + \\alpha_t^2}}$ | è¾“å‡ºå°ºåº¦æ ¡å‡†<br>â€¢ å°†ç½‘ç»œè¾“å‡º$F_\\theta$æ˜ å°„åˆ°å™ªå£°ç©ºé—´<br>â€¢ æ§åˆ¶é«˜é¢‘ç»†èŠ‚ç”Ÿæˆ | ä»$0$â†’$1$å•è°ƒé€’å¢           |\n",
    "| **c_skip(t)** | $\\frac{\\alpha_t}{\\sqrt{\\sigma_t^2 + \\alpha_t^2}}$ | æ®‹å·®è¿æ¥é—¨æ§<br>â€¢ ä¿ç•™ä½é¢‘ä¿¡å·æˆåˆ†<br>â€¢ é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±                      | ä»$1$â†’$0$å•è°ƒé€’å‡           |\n",
    " \n",
    "## å…³é”®æŠ€æœ¯åŸç† \n",
    " \n",
    "### 1. ååŒè®¾è®¡çº¦æŸ \n",
    "- **æ–¹å·®å®ˆæ’å®šç†**ï¼š  \n",
    "  $\\text{c\\_skip}^2(t) + \\text{c\\_out}^2(t) \\equiv 1$  \n",
    "  â†’ ç¡®ä¿ç½‘ç»œè¾“å‡º$\\epsilon_\\theta$çš„æ–¹å·®ä¸çœŸå®å™ªå£°$\\epsilon$åŒ¹é… \n",
    " \n",
    "- **æ¸è¿›ç‰¹æ€§**ï¼š  \n",
    "  ```python \n",
    "  # ä¼ªä»£ç æ¼”ç¤ºç³»æ•°åŠ¨æ€å˜åŒ–\n",
    "  def coefficient_behavior(t, T):\n",
    "      c_skip = alpha_t / sqrt(sigma_t**2 + alpha_t**2)  # éštå¢åŠ â†’0\n",
    "      c_out = sigma_t / sqrt(sigma_t**2 + alpha_t**2)   # éštå¢åŠ â†’1\n",
    "      return c_skip, c_out \n",
    "\n",
    "## ç®—æ³•ä¼˜åŠ¿åˆ†æ \n",
    "**è®­ç»ƒç¨³å®šæ€§**  \n",
    "- é€šè¿‡ç³»æ•°å‡½æ•°çš„è¾¹ç•Œæ§åˆ¶ï¼Œæœ‰æ•ˆæŠ‘åˆ¶æ¢¯åº¦å¼‚å¸¸ï¼ˆå¦‚çˆ†ç‚¸/æ¶ˆå¤±é—®é¢˜ï¼‰\n",
    " \n",
    "**æ”¶æ•›åŠ é€Ÿ**  \n",
    "- å®éªŒè¡¨æ˜å¯å‡å°‘30%-50%è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼ˆåŸºäºEDMè®ºæ–‡æ•°æ®ï¼‰\n",
    " \n",
    "**ç”Ÿæˆè´¨é‡æå‡**  \n",
    "- åœ¨FFHQæ•°æ®é›†æµ‹è¯•ä¸­ï¼ŒFIDåˆ†æ•°å¹³å‡æ”¹å–„15%-20%\n",
    " \n",
    "**æ¶æ„å…¼å®¹æ€§**  \n",
    "- å·²éªŒè¯é€‚é…UNetã€Transformerç­‰å¤šç§éª¨å¹²ç½‘ç»œ\n",
    "\n",
    "\n",
    "ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¬¡é‡‡æ ·ï¼Ÿâ€”â€”æ‰©æ•£æ¨¡å‹çš„è¿­ä»£å»å™ªæœ¬è´¨\n",
    "æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ ¸å¿ƒæ˜¯é€æ­¥å»å™ªçš„è¿­ä»£è¿‡ç¨‹ï¼š\n",
    "- å•æ¬¡å»å™ªçš„å±€é™æ€§ï¼šdenoise()å‡½æ•°ç¡®å®èƒ½é¢„æµ‹å™ªå£°ï¼Œä½†ç›´æ¥ç”¨å®ƒç”Ÿæˆçš„\"å»å™ªç»“æœ\"åªæ˜¯å½“å‰æ­¥éª¤çš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œå¯èƒ½åŒ…å«æ®‹ç•™å™ªå£°æˆ–ç»“æ„é”™è¯¯ã€‚\n",
    "- æ¸è¿›ä¿®æ­£æœºåˆ¶ï¼šé€šè¿‡å¤šæ¬¡é‡‡æ ·ï¼ˆå¦‚20~30æ­¥ï¼‰ï¼Œæ¨¡å‹å¯ä»¥ï¼š\n",
    "- é€æ­¥ä¿®æ­£é”™è¯¯ï¼šå‰ä¸€æ­¥çš„è¯¯å·®ä¼šåœ¨ä¸‹ä¸€æ­¥è¢«é‡æ–°è¯„ä¼°å’Œè°ƒæ•´ï¼ˆç±»ä¼¼ç”»å®¶åå¤ä¿®æ”¹è‰ç¨¿ï¼‰ã€‚\n",
    "- æ§åˆ¶æ”¶æ•›ç¨³å®šæ€§ï¼šå¤§å™ªå£°æ­¥ï¼ˆæ—©æœŸï¼‰ç²—è°ƒæ•´ä½“ç»“æ„ï¼Œå°å™ªå£°æ­¥ï¼ˆåæœŸï¼‰ç»†åŒ–ç»†èŠ‚\n",
    "\n",
    "\n",
    "# æ‰©æ•£æ¨¡å‹é‡‡æ ·æ–¹æ³•è¯¦è§£\n",
    " \n",
    "## æ¬§æ‹‰é‡‡æ ·æ–¹æ³•ï¼ˆEulerï¼‰\n",
    "æ¬§æ‹‰é‡‡æ ·å™¨æ˜¯æœ€ç®€å•çš„æ•°å€¼è§£æ³•ï¼ŒåŸºäºç»å…¸ODEæ±‚è§£å™¨åŸç†ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨å·®å•†è¿‘ä¼¼å¾®åˆ†ï¼Œé€šè¿‡å½“å‰æ­¥çš„æ¢¯åº¦ç›´æ¥çº¿æ€§å¤–æ¨ä¸‹ä¸€æ­¥çš„å€¼ã€‚ç®—æ³•å…¬å¼å¯è¡¨ç¤ºä¸ºï¼š`x_{t-1} = x_t + Î”t * f(x_t, t)`ï¼Œå…¶ä¸­Î”tæ˜¯æ­¥é•¿ï¼Œf(x_t, t)æ˜¯å™ªå£°é¢„æµ‹å‡½æ•°ã€‚æ¬§æ‹‰æ–¹æ³•è®¡ç®—é‡å°ä½†ç²¾åº¦ä¸€èˆ¬ï¼Œå¯¹åˆå§‹æ¡ä»¶æ•æ„Ÿï¼Œå®¹æ˜“å‡ºç°æ•°å€¼ä¸ç¨³å®šç°è±¡ã€‚åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ƒé€šè¿‡å™ªå£°è®¡åˆ’é€æ­¥å‡å°‘å™ªå£°é‡ï¼Œæœ€ç»ˆç”Ÿæˆæ¸…æ™°å›¾åƒã€‚\n",
    " \n",
    "## æ¬§æ‹‰ç¥–å…ˆé‡‡æ ·å™¨ï¼ˆEuler aï¼‰\n",
    "æ¬§æ‹‰ç¥–å…ˆé‡‡æ ·å™¨æ˜¯å¸¦æœ‰éšæœºæ€§çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå±äºç¥–å…ˆé‡‡æ ·å™¨ç±»åˆ«ã€‚å…¶æ ¸å¿ƒåŸç†æ˜¯åœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­é¢å¤–æ·»åŠ éšæœºå™ªå£°ï¼Œå…¬å¼ä¸ºï¼š`x_{t-1} = x_t + Î”t * f(x_t, t) + Ïƒ_t * Îµ`ï¼Œå…¶ä¸­Îµæ˜¯ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·çš„å™ªå£°ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç”Ÿæˆç»“æœå…·æœ‰ä¸å¯æ”¶æ•›çš„ç‰¹æ€§ï¼Œæ¯æ¬¡é‡‡æ ·éƒ½ä¼šäº§ç”Ÿéšæœºåå·®ï¼Œé€‚åˆéœ€è¦åˆ›é€ æ€§å’Œå¤šæ ·æ€§çš„åœºæ™¯ã€‚ä¸æ ‡å‡†æ¬§æ‹‰æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å»å™ªè¿‡ç¨‹ä¸­ä¼šå‡å»\"æ¯”åº”è¯¥æ›´å¤š\"çš„å™ªå£°å†è¡¥å…¥æ–°å™ªå£°ã€‚\n",
    " \n",
    "## Heunæ–¹æ³• \n",
    "Heunæ–¹æ³•æ˜¯æ¬§æ‹‰æ³•çš„äºŒé˜¶æ”¹è¿›ç‰ˆæœ¬ï¼Œé‡‡ç”¨é¢„æµ‹-æ ¡æ­£ç­–ç•¥æé«˜ç²¾åº¦ã€‚å…¶ç®—æ³•åˆ†ä¸ºä¸¤æ­¥ï¼š\n",
    "1. é¦–å…ˆè®¡ç®—é¢„æµ‹å€¼ï¼š`y* = x_t + Î”t * f(x_t, t)`\n",
    "2. ç„¶åæ ¡æ­£æ­¥éª¤ï¼š`x_{t-1} = x_t + Î”t/2 * [f(x_t, t) + f(y*, t+Î”t)]`\n",
    " \n",
    "æ ¸å¿ƒåŸç†æ˜¯é€šè¿‡ä¸¤æ¬¡å‡½æ•°è¯„ä¼°æ¥è·å¾—æ›´å‡†ç¡®çš„æ–œç‡ä¼°è®¡ï¼Œè™½ç„¶è®¡ç®—é‡æ˜¯æ¬§æ‹‰æ³•çš„ä¸¤å€ï¼Œä½†èƒ½æ˜¾è‘—å‡å°‘æˆªæ–­è¯¯å·®ã€‚åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒHeunå°¤å…¶é€‚åˆéœ€è¦é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„åœºæ™¯ã€‚ \n",
    " \n",
    "## çº¿æ€§å¤šæ­¥æ³•ï¼ˆLMSï¼‰\n",
    "çº¿æ€§å¤šæ­¥æ³•ï¼ˆLMSï¼‰å±äºç»å…¸ODEæ±‚è§£å™¨ï¼Œå…¶æ ¸å¿ƒåŸç†æ˜¯åˆ©ç”¨å‰å‡ æ­¥çš„å†å²ä¿¡æ¯æ¥æé«˜å½“å‰æ­¥çš„ä¼°è®¡ç²¾åº¦ã€‚åŸºæœ¬å…¬å¼ä¸ºï¼š`x_{t-1} = x_t + Î”t * Î£(Î±_i * f(x_{t-i}, t-i))`ï¼Œå…¶ä¸­Î±_iæ˜¯åŠ æƒç³»æ•°ã€‚ä¸æ¬§æ‹‰æ³•ç›¸æ¯”ï¼ŒLMSé€šè¿‡å¤šæ­¥ä¿¡æ¯äº¤æ¢è·å¾—æ›´ç¨³å¥çš„æ±‚è§£æ•ˆæœï¼Œåœ¨ä¿æŒç›¸åŒè®¡ç®—é€Ÿåº¦çš„åŒæ—¶æä¾›æ›´é«˜å‡†ç¡®æ€§ã€‚åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒLMSç‰¹åˆ«é€‚åˆéœ€è¦å¹³è¡¡é€Ÿåº¦ä¸è´¨é‡çš„åœºæ™¯ï¼Œèƒ½æœ‰æ•ˆæŠ‘åˆ¶æ•°å€¼æŒ¯è¡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc091a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_data = xb.std()\n",
    "sig_data = 0.66\n",
    "\n",
    "def scalings(sig):\n",
    "    totvar = sig**2+sig_data**2\n",
    "    # c_skip,c_out,c_in\n",
    "    return sig_data**2/totvar,sig*sig_data/totvar.sqrt(),1/totvar.sqrt()\n",
    "\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    # sig = (0.3 + 0.7*torch.rand([len(x0)])).log().to(x0)   # æ›´ç¨³å®šçš„åˆ†å¸ƒï¼Œé¿å…æç«¯å€¼\n",
    "    sig = (torch.randn([len(x0)])*1.2-1.2).exp().to(x0).reshape(-1,1,1,1)\n",
    "    noise = torch.randn_like(x0, device=device)\n",
    "    c_skip,c_out,c_in = scalings(sig)\n",
    "    noised_input = x0 + noise*sig\n",
    "    target = (x0-c_skip*noised_input)/c_out\n",
    "    return (noised_input*c_in,sig.squeeze()),target\n",
    "\n",
    "sz = (2048,1,32,32)\n",
    "\n",
    "def denoise(model, x, sig):\n",
    "    c_skip,c_out,c_in = scalings(sig)\n",
    "    return model((x*c_in, sig))*c_out + x*c_skip\n",
    "\n",
    "def get_ancestral_step(sigma_from, sigma_to, eta=1.):\n",
    "    if not eta: return sigma_to, 0.\n",
    "    var_to,var_from = sigma_to**2,sigma_from**2\n",
    "    sigma_up = min(sigma_to, eta * (var_to * (var_from-var_to)/var_from)**0.5)\n",
    "    return (var_to-sigma_up**2)**0.5, sigma_up\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_euler_ancestral(x, sigs, i, model, eta=1.):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    denoised = denoise(model, x, sig)\n",
    "    sigma_down,sigma_up = get_ancestral_step(sig, sig2, eta=eta)\n",
    "    x = x + (x-denoised)/sig*(sigma_down-sig)\n",
    "    return x + torch.randn_like(x)*sigma_up\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_euler(x, sigs, i, model):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    denoised = denoise(model, x, sig)\n",
    "    return x + (x-denoised)/sig*(sig2-sig)\n",
    "\n",
    "def sigmas_karras(n, sigma_min=0.01, sigma_max=80., rho=7., device='cpu'):\n",
    "    ramp = torch.linspace(0, 1, n)\n",
    "    min_inv_rho = sigma_min**(1/rho)\n",
    "    max_inv_rho = sigma_max**(1/rho)\n",
    "    sigmas = (max_inv_rho + ramp * (min_inv_rho-max_inv_rho))**rho\n",
    "    return torch.cat([sigmas, tensor([0.])]).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_heun(x, sigs, i, model, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    n = len(sigs)\n",
    "    gamma = min(s_churn/(n-1), 2**0.5-1) if s_tmin<=sig<=s_tmax else 0.\n",
    "    eps = torch.randn_like(x) * s_noise\n",
    "    sigma_hat = sig * (gamma+1)\n",
    "    if gamma > 0: x = x + eps * (sigma_hat**2-sig**2)**0.5\n",
    "    denoised = denoise(model, x, sig)\n",
    "    d = (x-denoised)/sig\n",
    "    dt = sig2-sigma_hat\n",
    "    x_2 = x + d*dt\n",
    "    if sig2==0: return x_2\n",
    "    denoised_2 = denoise(model, x_2, sig2)\n",
    "    d_2 = (x_2-denoised_2)/sig2\n",
    "    d_prime = (d+d_2)/2\n",
    "    return x + d_prime*dt\n",
    "\n",
    "def sample(sampler, model, steps=100, sigma_max=80., **kwargs):\n",
    "    preds = []\n",
    "    x = torch.randn(sz).to(model.device)*sigma_max\n",
    "    sigs = sigmas_karras(steps, device=model.device, sigma_max=sigma_max)\n",
    "    for i in range(len(sigs)-1):\n",
    "        x = sampler(x, sigs, i, model, **kwargs)\n",
    "        preds.append(x)\n",
    "    return preds\n",
    "\n",
    "def linear_multistep_coeff(order, t, i, j):\n",
    "    if order-1 > i: raise ValueError(f'Order {order} too high for step {i}')\n",
    "    def fn(tau):\n",
    "        prod = 1.\n",
    "        for k in range(order):\n",
    "            if j == k: continue\n",
    "            prod *= (tau-t[i-k]) / (t[i-j]-t[i-k])\n",
    "        return prod\n",
    "    return integrate.quad(fn, t[i], t[i+1], epsrel=1e-4)[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_lms(model, steps=100, order=4, sigma_max=80.):\n",
    "    preds = []\n",
    "    x = torch.randn(sz).to(model.device)*sigma_max\n",
    "    sigs = sigmas_karras(steps, device=model.device, sigma_max=sigma_max)\n",
    "    ds = []\n",
    "    for i in range(len(sigs)-1):\n",
    "        sig = sigs[i]\n",
    "        denoised = denoise(model, x, sig)\n",
    "        d = (x-denoised)/sig\n",
    "        ds.append(d)\n",
    "        if len(ds) > order: ds.pop(0)\n",
    "        cur_order = min(i+1, order)\n",
    "        coeffs = [linear_multistep_coeff(cur_order, sigs, i, j) for j in range(cur_order)]\n",
    "        x = x + sum(coeff*d for coeff, d in zip(coeffs, reversed(ds)))\n",
    "        preds.append(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e258b",
   "metadata": {},
   "source": [
    "# äº¤å‰å·ç§¯ï¼ˆCross Convolutionï¼‰æŠ€æœ¯è¯¦è§£ \n",
    " \n",
    "## æ ¸å¿ƒå®šä¹‰\n",
    "äº¤å‰å·ç§¯æ˜¯ä¸€ç§æ”¹è¿›çš„å·ç§¯æ“ä½œèŒƒå¼ï¼Œé€šè¿‡ç‰¹å¾äº¤å‰æœºåˆ¶å¢å¼ºä¼ ç»Ÿå·ç§¯çš„ç‰¹å¾äº¤äº’èƒ½åŠ›ã€‚\n",
    "\"CrossConv\"çš„å‘½åä¸»è¦æºè‡ªå…¶**è·¨ç‰¹å¾äº¤äº’**çš„è®¾è®¡ç†å¿µï¼š\n",
    "1. **è·¨å±‚è¿æ¥**ï¼šé€šè¿‡ResBlockå®ç°ç‰¹å¾è·¨å±‚èåˆï¼ˆè·³è·ƒè¿æ¥ï¼‰\n",
    "2. **è·¨æ„Ÿå—é‡**ï¼šç»„åˆæ®‹å·®å—ä¸æ ‡å‡†å·ç§¯å½¢æˆæ··åˆæ„Ÿå—é‡ \n",
    "3. **è·¨æ¨¡æ€å¤„ç†**ï¼šåœ¨ç‰¹å¾ç©ºé—´ä¸­å®ç°ä¸åŒæŠ½è±¡å±‚æ¬¡çš„ä¿¡æ¯äº¤å‰ \n",
    " \n",
    "## å…¸å‹ç»“æ„å¯¹æ¯” \n",
    "| æ¨¡å—ç±»å‹     | è¿æ¥æ–¹å¼              | ç‰¹å¾äº¤äº’æ¨¡å¼       |\n",
    "|--------------|-----------------------|--------------------|\n",
    "| æ™®é€šå·ç§¯      | çº¯åºåˆ—åŒ–              | å•å‘æµåŠ¨           |\n",
    "| **CrossConv** | æ®‹å·®+å·ç§¯äº¤å‰         | åŒå‘ç‰¹å¾èåˆ       |\n",
    "| DenseBlock   | å¯†é›†è¿æ¥              | å…¨äº’è¿             |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_conv(nf, act, norm):\n",
    "    return nn.Sequential(\n",
    "        ResBlock(nf, nf, act=act, norm=norm),\n",
    "        nn.Conv2d(nf, nf, 3, padding=1)\n",
    "    )\n",
    "\n",
    "class TinyUnet(nn.Module):\n",
    "    def __init__(self, act=act_gr, nfs=(32,64,128,256,512,1024), norm=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "#         åˆå§‹åŒ–æ¨¡å‹çš„å„ä¸ªéƒ¨åˆ†ï¼š\n",
    "# self.start ï¼šä¸€ä¸ªæ®‹å·®å—ï¼Œç”¨äºå¤„ç†è¾“å…¥å›¾åƒï¼Œè¾“å…¥é€šé“æ•°ä¸º 3ï¼Œè¾“å‡ºé€šé“æ•°ä¸º nfs[0]ã€‚\n",
    "# self.dn ï¼šä¸€ä¸ª nn.ModuleListï¼ŒåŒ…å«å¤šä¸ªæ®‹å·®å—ï¼Œç”¨äºä¸‹é‡‡æ ·è¿‡ç¨‹ï¼Œæ¯ä¸ªæ®‹å·®å—çš„æ­¥é•¿ä¸º 2ã€‚\n",
    "# self.xs ï¼šä¸€ä¸ª nn.ModuleListï¼ŒåŒ…å«å¤šä¸ª cross_conv æ¨¡å—ï¼Œç”¨äºè·¨è¿æ¥ã€‚\n",
    "# self.up ï¼šä¸€ä¸ª nn.ModuleListï¼ŒåŒ…å«å¤šä¸ªä¸Šé‡‡æ ·å—ï¼ˆup_blockï¼‰å’Œä¸€ä¸ªæ®‹å·®å—ï¼Œç”¨äºä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚\n",
    "# self.end ï¼šä¸€ä¸ªæ®‹å·®å—ï¼Œç”¨äºè¾“å‡ºæœ€ç»ˆç»“æœï¼Œè¾“å…¥å’Œè¾“å‡ºé€šé“æ•°å‡ä¸º 3ã€‚\n",
    "        self.start = ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)\n",
    "        self.dn = nn.ModuleList([ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "                                 for i in range(len(nfs)-1)])\n",
    "        self.xs = nn.ModuleList([cross_conv(nfs[i], act, norm)\n",
    "                                 for i in range(len(nfs)-1,0,-1)])\n",
    "        self.xs += [cross_conv(nfs[0], act, norm)]\n",
    "        self.up = nn.ModuleList([up_block(nfs[i], nfs[i-1], act=act, norm=norm)\n",
    "                                 for i in range(len(nfs)-1,0,-1)])\n",
    "        self.up += [ResBlock(nfs[0], 3, act=act, norm=norm)]\n",
    "        self.end = ResBlock(3, 3, act=nn.Identity, norm=norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # å°†è¾“å…¥ x å­˜å‚¨åœ¨ layers åˆ—è¡¨ä¸­ã€‚\n",
    "        # é€šè¿‡ self.start å¤„ç†è¾“å…¥ã€‚\n",
    "        # è¿›è¡Œä¸‹é‡‡æ ·è¿‡ç¨‹ï¼Œå°†æ¯ä¸ªä¸‹é‡‡æ ·é˜¶æ®µçš„è¾“å‡ºå­˜å‚¨åœ¨ layers åˆ—è¡¨ä¸­ã€‚\n",
    "        # è¿›è¡Œä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œåœ¨æ¯ä¸ªä¸Šé‡‡æ ·é˜¶æ®µï¼Œå°†è·¨è¿æ¥çš„è¾“å‡ºåŠ åˆ°å½“å‰ç‰¹å¾å›¾ä¸Šã€‚\n",
    "        # å°†æœ€ç»ˆç‰¹å¾å›¾ä¸è¾“å…¥ç›¸åŠ ï¼Œå¹¶é€šè¿‡ self.end è¾“å‡ºæœ€ç»ˆç»“æœã€‚\n",
    "        layers = []\n",
    "        layers.append(x)\n",
    "        x = self.start(x)\n",
    "        for i,l in enumerate(self.dn):\n",
    "            layers.append(x)\n",
    "            x = l(x)\n",
    "        n = len(layers)\n",
    "        for i,l in enumerate(self.up):\n",
    "            if i!=0: x += self.xs[i](layers[n-i])\n",
    "            x = l(x)\n",
    "        return self.end(x+layers[0])\n",
    "\n",
    "# class TfmDS:\n",
    "#     def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n",
    "#     def __len__(self): return len(self.ds)\n",
    "#     def __getitem__(self, i):\n",
    "#         item = self.ds[i]\n",
    "#         return self.tfmx(item),self.tfmy(item)\n",
    "\n",
    "# def denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clamp(0,1)\n",
    "\n",
    "# def tfmx(x, erase=True):\n",
    "#     x = TF.resize(x, (32,32))[None]\n",
    "#     x = F.interpolate(x, scale_factor=2)\n",
    "#     if erase: x = rand_erase(x)\n",
    "#     return x[0]\n",
    "\n",
    "# tds = TinyDS(path/'train')\n",
    "# vds = TinyDS(path/'val')\n",
    "\n",
    "# tfm_tds = TfmDS(tds, tfmx)\n",
    "# tfm_vds = TfmDS(vds, partial(tfmx, erase=False))\n",
    "\n",
    "# dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94859a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd96157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)#å‡è®¾æ¯ä¸ªå…ƒç´ ä¸ºiid(mean = 0, var = 1)ï¼Œåˆ™QK^Tçš„æ–¹å·®ä¸ºniï¼Œæ‰€ä»¥æ­¤å¤„ç¼©æ”¾å›1\n",
    "        self.norm = nn.GroupNorm(1, ni)\n",
    "        self.q = nn.Linear(ni, ni)\n",
    "        self.k = nn.Linear(ni, ni)\n",
    "        self.v = nn.Linear(ni, ni)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v #æ­¤å¤„-1ä»…è¡¨ç¤ºä½¿ç”¨dim = -1è¿›è¡Œå½’ä¸€åŒ–ï¼Œå³å¯¹æ¯è¡Œçš„ Key åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œ@vå¯¹Valueå‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œ\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp\n",
    "\n",
    "def cp_parms(a,b):\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias\n",
    "\n",
    "sa = SelfAttention(32)\n",
    "\n",
    "# åˆå§‹åŒ– Attention æ¨¡å— \n",
    "at = Attention(\n",
    "    query_dim=32,  # è¾“å…¥ç»´åº¦ \n",
    "    heads=1,  # æ³¨æ„åŠ›å¤´æ•° \n",
    "    dim_head=32,  # æ¯ä¸ªå¤´çš„ç»´åº¦ \n",
    "    cross_attention_norm = \"group_norm\",\n",
    "    norm_num_groups=1  # å½’ä¸€åŒ–ç»„æ•° \n",
    ")\n",
    " \n",
    "# æºå’Œç›®æ ‡å±æ€§ \n",
    "src = sa.q, sa.k, sa.v, sa.proj,  sa.norm  \n",
    "dst = at.to_q,  at.to_k,  at.to_v,  at.to_out,  at.norm_cross  \n",
    " \n",
    "# å¤åˆ¶å‚æ•° \n",
    "for s, d in zip(src, dst):\n",
    "    print(s,d)\n",
    "    cp_parms(s, d)\n",
    "\n",
    "\n",
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale = math.sqrt(ni/nheads)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp\n",
    " \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads  = nheads \n",
    "        self.scale  = math.sqrt(dim  / nheads)\n",
    "        \n",
    "        # Query æ¥è‡ªè¾“å…¥åºåˆ—Aï¼ŒKey/Value æ¥è‡ªåºåˆ—B\n",
    "        self.to_q  = nn.Linear(dim, dim)  # ä»…å¯¹QueryåšæŠ•å½± \n",
    "        self.to_kv  = nn.Linear(dim, dim * 2)  # åŒæ—¶ç”ŸæˆKeyå’ŒValue \n",
    "        \n",
    "        self.proj  = nn.Linear(dim, dim)  # è¾“å‡ºæŠ•å½±å±‚ \n",
    " \n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:      Queryåºåˆ— [batch, seq_len_q, dim]\n",
    "            context: Key/Valueåºåˆ— [batch, seq_len_kv, dim]\n",
    "        Returns:\n",
    "            è¾“å‡ºåºåˆ— [batch, seq_len_q, dim]\n",
    "        \"\"\"\n",
    "        b, nq, d = x.shape  \n",
    "        _, nkv, _ = context.shape  \n",
    "        \n",
    "        # 1. ç”ŸæˆQ/K/V\n",
    "        q = self.to_q(x)   # [b, nq, dim]\n",
    "        kv = self.to_kv(context)   # [b, nkv, dim*2]\n",
    "        k, v = torch.chunk(kv,  2, dim=-1)  # å„[b, nkv, dim]\n",
    "        \n",
    "        # 2. å¤šå¤´æ‹†åˆ† \n",
    "        q = rearrange(q, 'b nq (h d) -> (b h) nq d', h=self.nheads)   # [b*h, nq, d_h]\n",
    "        k = rearrange(k, 'b nkv (h d) -> (b h) nkv d', h=self.nheads)   # [b*h, nkv, d_h]\n",
    "        v = rearrange(v, 'b nkv (h d) -> (b h) nkv d', h=self.nheads)   # [b*h, nkv, d_h]\n",
    "        \n",
    "        # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° \n",
    "        attn = (q @ k.transpose(-2,  -1)) / self.scale   # [b*h, nq, nkv]\n",
    "        attn = attn.softmax(dim=-1) \n",
    "        \n",
    "        # 4. åŠ æƒæ±‚å’Œ\n",
    "        out = attn @ v  # [b*h, nq, d_h]\n",
    "        out = rearrange(out, '(b h) nq d -> b nq (h d)', h=self.nheads)   # [b, nq, dim]\n",
    "        \n",
    "        # 5. è¾“å‡ºæŠ•å½± \n",
    "        return self.proj(out) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3cc4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237a6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8688709c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
